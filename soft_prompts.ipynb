{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rohrl/llm_shenanigans/blob/main/soft_prompts.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# based on https://github.com/kipgparker/soft-prompt-tuning/blob/main/example.ipynb"
      ],
      "metadata": {
        "id": "eEHnZz7DCAcT"
      },
      "execution_count": 181,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 182,
      "metadata": {
        "id": "ZTriOPpNB5cB"
      },
      "outputs": [],
      "source": [
        "from transformers import GPT2LMHeadModel, GPT2TokenizerFast"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 183,
      "metadata": {
        "id": "Ur3z7dliB5cC"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class SoftEmbedding(nn.Module):\n",
        "    def __init__(self,\n",
        "                wte: nn.Embedding,\n",
        "                n_tokens: int = 10,\n",
        "                random_range: float = 0.5,\n",
        "                initialize_from_vocab: bool = True):\n",
        "        \"\"\"appends learned embedding to\n",
        "\n",
        "        Args:\n",
        "            wte (nn.Embedding): original transformer word embedding\n",
        "            n_tokens (int, optional): number of tokens for task. Defaults to 10.\n",
        "            random_range (float, optional): range to init embedding (if not initialize from vocab). Defaults to 0.5.\n",
        "            initialize_from_vocab (bool, optional): initalizes from default vocab. Defaults to True.\n",
        "        \"\"\"\n",
        "        super(SoftEmbedding, self).__init__()\n",
        "        self.wte = wte\n",
        "        self.n_tokens = n_tokens\n",
        "        self.learned_embedding = nn.parameter.Parameter(self.initialize_embedding(wte,\n",
        "                                                                               n_tokens,\n",
        "                                                                               random_range,\n",
        "                                                                               initialize_from_vocab))\n",
        "\n",
        "    def initialize_embedding(self,\n",
        "                             wte: nn.Embedding,\n",
        "                             n_tokens: int = 10,\n",
        "                             random_range: float = 0.5,\n",
        "                             initialize_from_vocab: bool = True):\n",
        "        \"\"\"initializes learned embedding\n",
        "\n",
        "        Args:\n",
        "            same as __init__\n",
        "\n",
        "        Returns:\n",
        "            torch.float: initialized using original schemes\n",
        "        \"\"\"\n",
        "        if initialize_from_vocab:\n",
        "            # this takes first n_tokens words from vocab and uses as init of learnt embeddings\n",
        "            return self.wte.weight[:n_tokens].clone().detach()\n",
        "        return torch.FloatTensor(n_tokens, wte.weight.size(1)).uniform_(-random_range, random_range)\n",
        "\n",
        "    def forward(self, tokens):\n",
        "        \"\"\"run forward pass\n",
        "\n",
        "        Args:\n",
        "            tokens (torch.long): input tokens before encoding\n",
        "\n",
        "        Returns:\n",
        "            torch.float: encoding of text concatenated with learned task specifc embedding\n",
        "        \"\"\"\n",
        "        # below line means that first n_tokens tokens will be ignored (?)\n",
        "        input_embedding = self.wte(tokens[:, self.n_tokens:])\n",
        "        learned_embedding = self.learned_embedding.repeat(input_embedding.size(0), 1, 1)\n",
        "        return torch.cat([learned_embedding, input_embedding], 1)"
      ],
      "metadata": {
        "id": "CZhoLYhJCE0D"
      },
      "execution_count": 184,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 185,
      "metadata": {
        "id": "EPcUhdnWB5cC"
      },
      "outputs": [],
      "source": [
        "n_tokens = 3 # 20\n",
        "initialize_from_vocab = False  # True"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UVK1UxLC5Lhj"
      },
      "execution_count": 185,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 186,
      "metadata": {
        "id": "XguFZWJ0B5cC"
      },
      "outputs": [],
      "source": [
        "tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n",
        "model = GPT2LMHeadModel.from_pretrained('gpt2')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.get_input_embeddings()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UQxAfk5b5L7i",
        "outputId": "e7d709e7-857a-4f42-9410-d32fa6675d9c"
      },
      "execution_count": 187,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Embedding(50257, 768)"
            ]
          },
          "metadata": {},
          "execution_count": 187
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N-SdU4762jY-",
        "outputId": "c6019327-0c89-426d-8aa8-998de22fd5a4"
      },
      "execution_count": 188,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GPT2TokenizerFast(name_or_path='gpt2', vocab_size=50257, model_max_length=1024, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
              "\t50256: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
              "}"
            ]
          },
          "metadata": {},
          "execution_count": 188
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 189,
      "metadata": {
        "id": "0t-TQM4jB5cC"
      },
      "outputs": [],
      "source": [
        "s_wte = SoftEmbedding(model.get_input_embeddings(),\n",
        "                      n_tokens=n_tokens,\n",
        "                      initialize_from_vocab=initialize_from_vocab)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "s_wte"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0xtCVD-U2qzW",
        "outputId": "8f820a12-c6d1-4b6a-b7e1-436492be681f"
      },
      "execution_count": 190,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SoftEmbedding(\n",
              "  (wte): Embedding(50257, 768)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 190
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 191,
      "metadata": {
        "id": "JTL4XET5B5cC"
      },
      "outputs": [],
      "source": [
        "model.set_input_embeddings(s_wte)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LlSpJmei2qP4"
      },
      "execution_count": 191,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = tokenizer(\"May the force be\", return_tensors=\"pt\")\n"
      ],
      "metadata": {
        "id": "4F8-pUf5Wqii"
      },
      "execution_count": 192,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8lZGnzHsW3S5",
        "outputId": "45e8e69e-0239-4ddc-ad9f-a282a1df2e84"
      },
      "execution_count": 193,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': tensor([[6747,  262, 2700,  307]]), 'attention_mask': tensor([[1, 1, 1, 1]])}"
            ]
          },
          "metadata": {},
          "execution_count": 193
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.decode(inputs.input_ids.squeeze(), skip_special_tokens=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "y7yLnfQ436M-",
        "outputId": "f4250d00-25a8-460d-ad27-07e7c62a8503"
      },
      "execution_count": 194,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'May the force be'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 194
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 195,
      "metadata": {
        "id": "yzc9vtg3B5cC"
      },
      "outputs": [],
      "source": [
        "\n",
        "# need to pad attention_mask and input_ids to be full seq_len + n_learned_tokens\n",
        "# even though it does not matter what you pad input_ids with, it's just to make HF happy\n",
        "# more exp: the SoftEmbedding implementation ignores first n_tokens of input tokens so this padding is to insert them at the beginning (and also make consistent with attention_mask length)\n",
        "# Padding is made of repeated \"unk_token\" (but it doesn't matter as it's ignored)\n",
        "inputs['input_ids'] = torch.cat([torch.full((1,n_tokens), tokenizer.unk_token_id), inputs['input_ids']], 1)\n",
        "inputs['attention_mask'] = torch.cat([torch.full((1,n_tokens), 1), inputs['attention_mask']], 1)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(inputs)\n",
        "print(tokenizer.decode(inputs.input_ids.squeeze(), skip_special_tokens=False))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NVIVIsGL6cd9",
        "outputId": "c111ffdc-e1ae-480f-9de0-feb052ee2e8f"
      },
      "execution_count": 196,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input_ids': tensor([[50256, 50256, 50256,  6747,   262,  2700,   307]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1]])}\n",
            "<|endoftext|><|endoftext|><|endoftext|>May the force be\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# outputs = model(**inputs)\n",
        "\n",
        "new_out_tokens = 20\n",
        "curr_inputs = inputs\n",
        "\n",
        "for i in range(new_out_tokens):\n",
        "  outputs = model.generate(**inputs, max_length = curr_inputs.input_ids.size(1) + 1)\n",
        "  new_token_id = outputs.squeeze()[-1]\n",
        "\n",
        "  # add the new token to inputs and repeat\n",
        "  curr_inputs['input_ids'] = torch.cat([curr_inputs['input_ids'], torch.full((1, 1), new_token_id)], 1)\n",
        "  curr_inputs['attention_mask'] = torch.cat([curr_inputs['attention_mask'], torch.full((1,1), 1)], 1)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s9zyLRpvWsRC",
        "outputId": "cf41d440-4575-41f2-df16-ab177a0f5de0"
      },
      "execution_count": 197,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# print(outputs.logits.shape)\n",
        "print(outputs)\n",
        "\n",
        "predicted_token_ids = outputs.squeeze()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CPuLynsUXb5p",
        "outputId": "a1d0169d-1bf6-4555-fae8-b23edcfc614f"
      },
      "execution_count": 198,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[50256, 50256, 50256,  6747,   262,  2700,   307,   351,   345,    13,\n",
            "           198,   198,   464,   691,  1517,   326,   338,   407,   287,   262,\n",
            "           835,   286,   262,  1621,   318,   262,  1109]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = tokenizer.decode(predicted_token_ids, skip_special_tokens=False) #[0]\n",
        "\n",
        "# Print the decoded text\n",
        "print(f\"|{text}|\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S5q_PWwgCPPI",
        "outputId": "b53d336f-8749-4b77-b59c-87850860b530"
      },
      "execution_count": 199,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "|<|endoftext|><|endoftext|><|endoftext|>May the force be with you.\n",
            "\n",
            "The only thing that's not in the way of the story is the fact|\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output_probs = torch.softmax(outputs.logits[0], dim=-1)\n",
        "predicted_token_ids = torch.argmax(output_probs, dim=-1)"
      ],
      "metadata": {
        "id": "Jdqj2JwYCYXm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159
        },
        "outputId": "7fa883cf-e1cb-4565-dcdb-e2a52f8a1122"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'Tensor' object has no attribute 'logits'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-71-bda2ead76f53>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0moutput_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mpredicted_token_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_probs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'Tensor' object has no attribute 'logits'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "47rAABJKCTBo"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}