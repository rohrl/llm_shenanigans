{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rohrl/llm_shenanigans/blob/main/soft_prompts.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# based on https://github.com/kipgparker/soft-prompt-tuning/blob/main/example.ipynb"
      ],
      "metadata": {
        "id": "eEHnZz7DCAcT"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FLMOazNLsgwP"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2ZrMFWkvsgA1"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install sentencepiece transformers accelerate einops"
      ],
      "metadata": {
        "id": "1Zde6dRjk8e0"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ZTriOPpNB5cB"
      },
      "outputs": [],
      "source": [
        "from transformers import GPT2LMHeadModel, GPT2TokenizerFast"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Ur3z7dliB5cC"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.set_default_device('cuda')"
      ],
      "metadata": {
        "id": "UwsFPl2TP5j0"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "XguFZWJ0B5cC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5aff4b66-d7eb-4cd2-9a9e-d25224126824"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n",
        "model = GPT2LMHeadModel.from_pretrained('gpt2', device_map=\"cuda\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HTLW-jE0hcjn",
        "outputId": "cb1676ac-33b8-43ae-abfd-f2a35d2af2aa"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda', index=0)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sanity check"
      ],
      "metadata": {
        "id": "oLIP2Myzhd0q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sanity_text = \"The capital of Australia\"\n",
        "sanity_output = model.generate(input_ids = tokenizer.encode(sanity_text, return_tensors=\"pt\"), max_length=10, num_return_sequences=1)\n",
        "print(\"==================\\n\" + tokenizer.decode(sanity_output[0], skip_special_tokens=True) + \"\\n==================\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lCDqd103hg3x",
        "outputId": "d5f3b525-06ce-45fe-960e-0299f713e74e"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==================\n",
            "The capital of Australia, Sydney, is home to\n",
            "==================\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Soft Embeddings"
      ],
      "metadata": {
        "id": "e-m2OeNuhh0T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SoftEmbedding(nn.Module):\n",
        "    def __init__(self,\n",
        "                wte: nn.Embedding,\n",
        "                n_tokens: int = 10,\n",
        "                random_range: float = 0.5,\n",
        "                initialize_from_vocab: bool = True):\n",
        "        \"\"\"appends learned embedding to\n",
        "\n",
        "        Args:\n",
        "            wte (nn.Embedding): original transformer word embedding\n",
        "            n_tokens (int, optional): number of tokens for task. Defaults to 10.\n",
        "            random_range (float, optional): range to init embedding (if not initialize from vocab). Defaults to 0.5.\n",
        "            initialize_from_vocab (bool, optional): initalizes from default vocab. Defaults to True.\n",
        "        \"\"\"\n",
        "        super(SoftEmbedding, self).__init__()\n",
        "        self.wte = wte\n",
        "        self.n_tokens = n_tokens\n",
        "        self.learned_embedding = nn.parameter.Parameter(self.initialize_embedding(wte,\n",
        "                                                                               n_tokens,\n",
        "                                                                               random_range,\n",
        "                                                                               initialize_from_vocab))\n",
        "\n",
        "    def initialize_embedding(self,\n",
        "                             wte: nn.Embedding,\n",
        "                             n_tokens: int = 10,\n",
        "                             random_range: float = 0.5,\n",
        "                             initialize_from_vocab: bool = True):\n",
        "        \"\"\"initializes learned embedding\n",
        "\n",
        "        Args:\n",
        "            same as __init__\n",
        "\n",
        "        Returns:\n",
        "            torch.float: initialized using original schemes\n",
        "        \"\"\"\n",
        "        if initialize_from_vocab:\n",
        "            # this takes first n_tokens words from vocab and uses as init of learnt embeddings\n",
        "            return self.wte.weight[:n_tokens].clone().detach()\n",
        "        return torch.FloatTensor(n_tokens, wte.weight.size(1)).uniform_(-random_range, random_range).to('cuda')\n",
        "\n",
        "    def forward(self, tokens):\n",
        "        \"\"\"run forward pass\n",
        "\n",
        "        Args:\n",
        "            tokens (torch.long): input tokens before encoding\n",
        "\n",
        "        Returns:\n",
        "            torch.float: encoding of text concatenated with learned task specifc embedding\n",
        "        \"\"\"\n",
        "        # below line means that first n_tokens tokens will be ignored (?)\n",
        "        input_embedding = self.wte(tokens[:, self.n_tokens:])\n",
        "        learned_embedding = self.learned_embedding.repeat(input_embedding.size(0), 1, 1)\n",
        "        return torch.cat([learned_embedding, input_embedding], 1)"
      ],
      "metadata": {
        "id": "CZhoLYhJCE0D"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "EPcUhdnWB5cC"
      },
      "outputs": [],
      "source": [
        "# How many soft prompt tokens do we want to use.\n",
        "num_soft_prompt_tokens = 5 # 20\n",
        "initialize_from_vocab = False  # True"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.get_input_embeddings()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UQxAfk5b5L7i",
        "outputId": "a1040fab-512e-4140-cb03-3e404ac8fe50"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Embedding(50257, 768)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N-SdU4762jY-",
        "outputId": "ca22fd31-1350-4c0e-9924-58dc3894c17d"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GPT2TokenizerFast(name_or_path='gpt2', vocab_size=50257, model_max_length=1024, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
              "\t50256: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
              "}"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "0t-TQM4jB5cC"
      },
      "outputs": [],
      "source": [
        "s_wte = SoftEmbedding(model.get_input_embeddings(),\n",
        "                      n_tokens = num_soft_prompt_tokens,\n",
        "                      initialize_from_vocab = initialize_from_vocab)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "s_wte"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0xtCVD-U2qzW",
        "outputId": "357abed5-c365-41e5-84d8-92d96a4e69cb"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SoftEmbedding(\n",
              "  (wte): Embedding(50257, 768)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "JTL4XET5B5cC"
      },
      "outputs": [],
      "source": [
        "model.set_input_embeddings(s_wte)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "a202U5jLbU0n"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "yzc9vtg3B5cC"
      },
      "outputs": [],
      "source": [
        "def prepend_with_soft_prompts_padding(inputs, num_soft_tokens, pad_token_id = tokenizer.unk_token_id, labels = None):\n",
        "    \"\"\"\n",
        "    Need to pad attention_mask and input_ids to be full seq_len + n_learned_tokens,\n",
        "    even though it does not matter what you pad input_ids with, it's just to make HF happy.\n",
        "    More exp: the SoftEmbedding implementation ignores first num_soft_prompt_tokens of input tokens so this padding is to insert them at the beginning (and also make consistent with attention_mask length)\n",
        "    Padding is made of repeated \"unk_token\" (but it doesn't matter as it's ignored).\n",
        "    \"\"\"\n",
        "    batch_size = inputs['input_ids'].size(0)\n",
        "\n",
        "    inputs['input_ids'] = torch.cat([torch.full((batch_size, num_soft_tokens), pad_token_id), inputs['input_ids']], 1)\n",
        "    inputs['attention_mask'] = torch.cat([torch.full((batch_size, num_soft_tokens), 1), inputs['attention_mask']], 1)\n",
        "\n",
        "    if labels is None:\n",
        "        return inputs\n",
        "    else:\n",
        "        labels = torch.cat([torch.full((batch_size, num_soft_tokens), pad_token_id), labels], 1)\n",
        "        return inputs, labels\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inference"
      ],
      "metadata": {
        "id": "rjXwnhSPbX9g"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_KiLlFaZpCKO"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = tokenizer(\"The capital of Australia\", return_tensors=\"pt\")\n"
      ],
      "metadata": {
        "id": "4F8-pUf5Wqii"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8lZGnzHsW3S5",
        "outputId": "68cc13ba-a9d2-4f70-89c0-030cbef57b7d"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': tensor([[ 464, 3139,  286, 4505]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1]], device='cuda:0')}"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.decode(inputs['input_ids'].squeeze(), skip_special_tokens=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "y7yLnfQ436M-",
        "outputId": "66474168-e9bb-4809-b590-923840a5bf95"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The capital of Australia'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = prepend_with_soft_prompts_padding(inputs, num_soft_prompt_tokens)\n",
        "\n",
        "print(inputs)\n",
        "print(tokenizer.decode(inputs['input_ids'].squeeze(), skip_special_tokens=False))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NVIVIsGL6cd9",
        "outputId": "2a8d67a0-8e4a-4f08-c6da-b0461368fdb9"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input_ids': tensor([[50256, 50256, 50256, 50256, 50256,   464,  3139,   286,  4505]],\n",
            "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')}\n",
            "<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>The capital of Australia\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# outputs = model(**inputs)\n",
        "\n",
        "new_out_tokens = 10\n",
        "curr_inputs = inputs\n",
        "\n",
        "new_token_id = 0\n",
        "outputs = torch.cat([inputs['input_ids'], torch.full((1, new_out_tokens), 0) ], 1)\n",
        "\n",
        "\n",
        "model.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "  for i in range(new_out_tokens):\n",
        "\n",
        "    # outputs = model.generate(**inputs, max_length = curr_inputs['input_ids'].size(1) + 1)\n",
        "    raw_outputs = model(**curr_inputs)\n",
        "    # print(raw_outputs.logits.shape)\n",
        "\n",
        "    # new_token_id = outputs.squeeze()[-1]\n",
        "    new_token_id = raw_outputs.logits[:,-1,:].argmax(axis=-1).item()\n",
        "    outputs[:, (-new_out_tokens+i)] = new_token_id\n",
        "    # print(outputs)\n",
        "\n",
        "    # add the new token to inputs and repeat\n",
        "    curr_inputs['input_ids'] = torch.cat([curr_inputs['input_ids'], torch.full((1, 1), new_token_id)], 1)\n",
        "    curr_inputs['attention_mask'] = torch.cat([curr_inputs['attention_mask'], torch.full((1,1), 1)], 1)\n",
        "\n"
      ],
      "metadata": {
        "id": "s9zyLRpvWsRC"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "prM4lP7vF_IH"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(outputs.logits.shape)\n",
        "print(outputs)\n",
        "\n",
        "predicted_token_ids = outputs.squeeze()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CPuLynsUXb5p",
        "outputId": "e8dc63e0-bf93-4a03-d420-78ba132fb1b5"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[50256, 50256, 50256, 50256, 50256,   464,  3139,   286,  4505,   318,\n",
            "         33452,    13, 33452,   318, 33452,    13, 33452,   318, 33452]],\n",
            "       device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = tokenizer.decode(predicted_token_ids, skip_special_tokens=False) #[0]\n",
        "\n",
        "# Print the decoded text\n",
        "print(f\"|{text}|\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S5q_PWwgCPPI",
        "outputId": "c1974717-8b1e-439d-f61f-6e3773972cec"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "|<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>The capital of Australia is Canberra. Canberra is Canberra. Canberra is Canberra|\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Jdqj2JwYCYXm"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training"
      ],
      "metadata": {
        "id": "iGJix7h3ajG5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import copy\n",
        "\n",
        "# this token is ignored in loss - used to mask remainder of output\n",
        "ignored_token_id = tokenizer.unk_token_id\n",
        "\n",
        "target = \"The capital of Australia is Canberra.\"\n",
        "\n",
        "target_tokens = tokenizer(target, return_tensors=\"pt\")\n",
        "target_len = target_tokens['input_ids'].size(1)\n",
        "\n",
        "print(target_tokens['input_ids'])\n",
        "\n",
        "# create the batch by repeating tokens, then in the loop mask endings\n",
        "\n",
        "target_tokens['input_ids'] = target_tokens['input_ids'].repeat(target_len - 1, 1)\n",
        "target_tokens['attention_mask'] = target_tokens['attention_mask'].repeat(target_len - 1, 1)\n",
        "\n",
        "# labels will be the next token, so clone and left-shift 1 hop\n",
        "labels = target_tokens['input_ids'].clone()\n",
        "labels = labels.roll(-1, dims=-1)\n",
        "\n",
        "# add masks\n",
        "for i in range(target_len - 1):\n",
        "  # pad right of i\n",
        "  #labels.append(target_tokens['input_ids'][i, i+1].item())\n",
        "  target_tokens['input_ids'][i, i+1:] = torch.full((1, target_len - i - 1), ignored_token_id)\n",
        "  labels[i, i+1:] = torch.full((1, target_len - i - 1), ignored_token_id)\n",
        "  target_tokens['attention_mask'][i, i+1:] = torch.full((1, target_len - i - 1), 0)\n",
        "\n",
        "# last token will never be fed as input so trim all tensors\n",
        "target_tokens['input_ids'] = target_tokens['input_ids'][:, :-1]\n",
        "target_tokens['attention_mask'] = target_tokens['attention_mask'][:, :-1]\n",
        "labels = labels[:, :-1]\n",
        "\n",
        "# Finally, pad inputs with soft prompts\n",
        "target_tokens, labels = prepend_with_soft_prompts_padding(target_tokens, num_soft_prompt_tokens, labels = labels)\n",
        "\n",
        "\n",
        "print(target_tokens['input_ids'])\n",
        "print(target_tokens['attention_mask'])\n",
        "print(labels)\n"
      ],
      "metadata": {
        "id": "47rAABJKCTBo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed48f278-a264-4bb5-ce1c-ca65847a2f2b"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[  464,  3139,   286,  4505,   318, 33452,    13]], device='cuda:0')\n",
            "tensor([[50256, 50256, 50256, 50256, 50256,   464, 50256, 50256, 50256, 50256,\n",
            "         50256],\n",
            "        [50256, 50256, 50256, 50256, 50256,   464,  3139, 50256, 50256, 50256,\n",
            "         50256],\n",
            "        [50256, 50256, 50256, 50256, 50256,   464,  3139,   286, 50256, 50256,\n",
            "         50256],\n",
            "        [50256, 50256, 50256, 50256, 50256,   464,  3139,   286,  4505, 50256,\n",
            "         50256],\n",
            "        [50256, 50256, 50256, 50256, 50256,   464,  3139,   286,  4505,   318,\n",
            "         50256],\n",
            "        [50256, 50256, 50256, 50256, 50256,   464,  3139,   286,  4505,   318,\n",
            "         33452]], device='cuda:0')\n",
            "tensor([[1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')\n",
            "tensor([[50256, 50256, 50256, 50256, 50256,  3139, 50256, 50256, 50256, 50256,\n",
            "         50256],\n",
            "        [50256, 50256, 50256, 50256, 50256,  3139,   286, 50256, 50256, 50256,\n",
            "         50256],\n",
            "        [50256, 50256, 50256, 50256, 50256,  3139,   286,  4505, 50256, 50256,\n",
            "         50256],\n",
            "        [50256, 50256, 50256, 50256, 50256,  3139,   286,  4505,   318, 50256,\n",
            "         50256],\n",
            "        [50256, 50256, 50256, 50256, 50256,  3139,   286,  4505,   318, 33452,\n",
            "         50256],\n",
            "        [50256, 50256, 50256, 50256, 50256,  3139,   286,  4505,   318, 33452,\n",
            "            13]], device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# freeze entire model, then unfreeze soft embeddings\n",
        "model.requires_grad_(False)\n",
        "s_wte.requires_grad_(True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-BzGQohtaO4s",
        "outputId": "02d69fd6-e93e-4153-a689-98da1c027d4c"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SoftEmbedding(\n",
              "  (wte): Embedding(50257, 768)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_loss(criterion, logits, labels):\n",
        "\n",
        "    logits_flat = logits.view(-1, logits.size(-1))\n",
        "    # print(logits_flat.shape)\n",
        "\n",
        "    labels_flat = labels.flatten()\n",
        "    # print(labels_flat.shape)\n",
        "\n",
        "    loss = criterion(logits_flat, labels_flat)\n",
        "\n",
        "    loss_per_batch = loss.mean()\n",
        "\n",
        "    return loss_per_batch"
      ],
      "metadata": {
        "id": "8JSc45v0UonR"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.train()\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(ignore_index = ignored_token_id, reduction='none')\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01) # Try Adam\n",
        "\n",
        "best_loss = 1e9\n",
        "best_soft_prompts = None\n",
        "\n",
        "# Train the model\n",
        "num_epochs = 200\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    # Forward pass\n",
        "    outputs = model(input_ids = target_tokens['input_ids'], attention_mask = target_tokens['attention_mask'])\n",
        "\n",
        "    # print(outputs.logits.shape) #, outputs.logits)\n",
        "    # print(labels.shape, labels)\n",
        "\n",
        "    loss = compute_loss(criterion, outputs.logits, labels)\n",
        "\n",
        "    # Backpropagation\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # Save best\n",
        "    if loss.item() < best_loss:\n",
        "        best_loss = loss.item()\n",
        "        best_soft_prompts = copy.deepcopy(s_wte) # .clone() fails :(\n",
        "        print('--- NEW BEST: Epoch: {}, Loss: {:.4f}'.format(epoch+1, best_loss))\n",
        "\n",
        "\n",
        "    # Print the loss\n",
        "    if epoch % 10 == 0:\n",
        "        print('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, loss.item()))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RF8Y3i5hcEji",
        "outputId": "93d8913d-298a-4cb8-80df-2754018f7da9"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- NEW BEST: Epoch: 1, Loss: 1.9366\n",
            "Epoch [1/200], Loss: 1.9366\n",
            "Epoch [11/200], Loss: 9.0823\n",
            "Epoch [21/200], Loss: 6.3038\n",
            "--- NEW BEST: Epoch: 29, Loss: 1.8657\n",
            "--- NEW BEST: Epoch: 31, Loss: 1.7750\n",
            "Epoch [31/200], Loss: 1.7750\n",
            "--- NEW BEST: Epoch: 32, Loss: 0.9168\n",
            "--- NEW BEST: Epoch: 35, Loss: 0.7546\n",
            "Epoch [41/200], Loss: 2.6753\n",
            "Epoch [51/200], Loss: 1.4778\n",
            "Epoch [61/200], Loss: 1.3136\n",
            "--- NEW BEST: Epoch: 64, Loss: 0.7347\n",
            "Epoch [71/200], Loss: 0.9173\n",
            "--- NEW BEST: Epoch: 74, Loss: 0.5162\n",
            "Epoch [81/200], Loss: 0.7370\n",
            "--- NEW BEST: Epoch: 91, Loss: 0.5002\n",
            "Epoch [91/200], Loss: 0.5002\n",
            "--- NEW BEST: Epoch: 92, Loss: 0.3778\n",
            "--- NEW BEST: Epoch: 93, Loss: 0.3711\n",
            "--- NEW BEST: Epoch: 96, Loss: 0.3488\n",
            "Epoch [101/200], Loss: 0.5596\n",
            "--- NEW BEST: Epoch: 103, Loss: 0.3432\n",
            "Epoch [111/200], Loss: 0.3456\n",
            "--- NEW BEST: Epoch: 117, Loss: 0.3326\n",
            "Epoch [121/200], Loss: 0.5032\n",
            "--- NEW BEST: Epoch: 124, Loss: 0.2376\n",
            "--- NEW BEST: Epoch: 125, Loss: 0.2256\n",
            "Epoch [131/200], Loss: 0.5356\n",
            "--- NEW BEST: Epoch: 132, Loss: 0.2232\n",
            "Epoch [141/200], Loss: 0.8148\n",
            "--- NEW BEST: Epoch: 143, Loss: 0.2006\n",
            "--- NEW BEST: Epoch: 151, Loss: 0.1495\n",
            "Epoch [151/200], Loss: 0.1495\n",
            "--- NEW BEST: Epoch: 155, Loss: 0.1439\n",
            "Epoch [161/200], Loss: 0.5589\n",
            "--- NEW BEST: Epoch: 169, Loss: 0.1200\n",
            "Epoch [171/200], Loss: 0.3569\n",
            "Epoch [181/200], Loss: 0.4573\n",
            "--- NEW BEST: Epoch: 186, Loss: 0.0899\n",
            "Epoch [191/200], Loss: 0.4306\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "best_soft_prompts"
      ],
      "metadata": {
        "id": "r3S2rjr0hb19",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "88a7698a-7511-4182-e379-2d6c31bef632"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SoftEmbedding(\n",
              "  (wte): Embedding(50257, 768)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the best soft prompts on the model.\n",
        "model.set_input_embeddings(best_soft_prompts)"
      ],
      "metadata": {
        "id": "oXti4K4HrEqu"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"===============================================================================================\")\n",
        "print(\">>>>> Now go back to Inference section and see what you get with trained soft prompts =] <<<<<<\")\n",
        "print(\"===============================================================================================\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xVcssWH5XZ2S",
        "outputId": "eab83ec0-cbfe-4ecd-a10f-e4ce36fb2e1c"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "===============================================================================================\n",
            ">>>>> Now go back to Inference section and see what you get with trained soft prompts =] <<<<<<\n",
            "===============================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Y3tnYi9MXaOz"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# USING HF LIBRARY\n",
        "# from transformers import TrainingArguments, Trainer\n",
        "\n",
        "# training_args = TrainingArguments(\n",
        "#     output_dir=\"./model_checkpoints\",  # Output directory for checkpoints\n",
        "#     num_train_epochs=3,  # Total number of training epochs\n",
        "#     per_device_train_batch_size=16,  # Batch size per device\n",
        "#     per_device_eval_batch_size=16,  # Batch size for evaluation\n",
        "#     warmup_steps=500,  # Number of warmup steps\n",
        "#     logging_steps=100,  # Number of steps between logging\n",
        "#     save_steps=1000,  # Number of steps between saving checkpoints\n",
        "#     evaluation_strategy=\"steps\",  # Evaluation strategy\n",
        "#     eval_steps=1000,  # Number of steps between evaluations\n",
        "# )\n",
        "\n",
        "# trainer = Trainer(\n",
        "#     model=model,  # The model to train\n",
        "#     args=training_args,  # Training arguments\n",
        "#     train_dataset=train_dataset,  # Training dataset\n",
        "#     eval_dataset=eval_dataset,  # Evaluation dataset\n",
        "# )\n",
        "\n",
        "# trainer.train()"
      ],
      "metadata": {
        "id": "-yhD7Et_XQmD"
      },
      "execution_count": 31,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}