{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rohrl/llm_shenanigans/blob/main/soft_prompts.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# based on https://github.com/kipgparker/soft-prompt-tuning/blob/main/example.ipynb"
      ],
      "metadata": {
        "id": "eEHnZz7DCAcT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentencepiece transformers accelerate einops"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Zde6dRjk8e0",
        "outputId": "fb24cff3-31e0-40de-e5df-7cb160e6597a"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (0.1.99)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.38.2)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.28.0)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (0.7.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.3)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.2)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.2.1+cu121)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.10.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.3)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ZTriOPpNB5cB"
      },
      "outputs": [],
      "source": [
        "from transformers import GPT2LMHeadModel, GPT2TokenizerFast, AutoModelForCausalLM, AutoTokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Ur3z7dliB5cC"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.set_default_device('cuda')"
      ],
      "metadata": {
        "id": "UwsFPl2TP5j0"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "XguFZWJ0B5cC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 171,
          "referenced_widgets": [
            "08f5a8ed158846d999b8e647f4bad9b5",
            "775a3884c9f6484db8bcb5fe2d366e6d",
            "38e1ea780b4e4753be04d997ade161f3",
            "29e2d02507d144ec8b2854997f0b75c5",
            "567d22ec0e904a7cb80f22544d3be080",
            "8d4d3f45454c4adfa05553359cce76d4",
            "96cb5d61eaa74e6db643b98d7c538741",
            "1be8877023634452bf640217bbbd6fe1",
            "1458f0e25041457c9c5bd483d43605d3",
            "d54c465a216343fcbf64409b940a02a0",
            "17872042afd74463ad5f6ae4b1cf2c5e"
          ]
        },
        "outputId": "ea726c1a-6ffa-4fa3-eed9-b947f9f93e25"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "08f5a8ed158846d999b8e647f4bad9b5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "# tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n",
        "# model = GPT2LMHeadModel.from_pretrained('gpt2', device_map=\"cuda\")\n",
        "\n",
        "# model_id = \"itsliupeng/llama2_7b_mmlu\"\n",
        "# model = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"cuda\")\n",
        "# tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\"microsoft/phi-2\", torch_dtype=\"auto\", device_map=\"cuda\", trust_remote_code=True)\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/phi-2\", trust_remote_code=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HTLW-jE0hcjn",
        "outputId": "78022f83-ccf9-4484-dcf0-83fd4e6a8adf"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda', index=0)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sanity check"
      ],
      "metadata": {
        "id": "oLIP2Myzhd0q"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uNB7j7UJ6etf"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_empty_input():\n",
        "    return {'input_ids': torch.empty(size=(1,0)).to(torch.int64), 'attention_mask': torch.empty(size=(1,0)).to(torch.int64)}"
      ],
      "metadata": {
        "id": "Enmki7qE59tt"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sanity_text = \"The capital of Australia is\"\n",
        "sanity_output = model.generate(input_ids = tokenizer.encode(sanity_text, return_tensors=\"pt\"), max_length=15, num_return_sequences=1)\n",
        "print(\"*******************************************\\n\" +\n",
        "      tokenizer.decode(sanity_output[0], skip_special_tokens=True) +\n",
        "      \"\\n*******************************************\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lCDqd103hg3x",
        "outputId": "c59eda24-b246-4446-8a0b-2d9f01d11a7e"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*******************************************\n",
            "The capital of Australia is Canberra.\n",
            "    How many more tickets should Is\n",
            "*******************************************\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# inputs = create_empty_input()\n",
        "# model.generate(inputs['input_ids'], max_length=10, num_return_sequences=1)\n",
        "# print(\"==================\\n\" + tokenizer.decode(sanity_output[0], skip_special_tokens=True) + \"\\n==================\\n\")"
      ],
      "metadata": {
        "id": "1dTAN8s76v8A"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "3_CV8MKh6d_x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Soft Embeddings"
      ],
      "metadata": {
        "id": "e-m2OeNuhh0T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SoftEmbedding(nn.Module):\n",
        "    def __init__(self,\n",
        "                wte: nn.Embedding,\n",
        "                n_tokens: int = 10,\n",
        "                random_range: float = 0.5,\n",
        "                initialize_from_vocab: bool = True):\n",
        "        \"\"\"appends learned embedding to\n",
        "\n",
        "        Args:\n",
        "            wte (nn.Embedding): original transformer word embedding\n",
        "            n_tokens (int, optional): number of tokens for task. Defaults to 10.\n",
        "            random_range (float, optional): range to init embedding (if not initialize from vocab). Defaults to 0.5.\n",
        "            initialize_from_vocab (bool, optional): initalizes from default vocab. Defaults to True.\n",
        "        \"\"\"\n",
        "        super(SoftEmbedding, self).__init__()\n",
        "        self.wte = wte\n",
        "        self.n_tokens = n_tokens\n",
        "        self.learned_embedding = nn.parameter.Parameter(self.initialize_embedding(wte,\n",
        "                                                                               n_tokens,\n",
        "                                                                               random_range,\n",
        "                                                                               initialize_from_vocab))\n",
        "\n",
        "    def initialize_embedding(self,\n",
        "                             wte: nn.Embedding,\n",
        "                             n_tokens: int = 10,\n",
        "                             random_range: float = 0.5,\n",
        "                             initialize_from_vocab: bool = True):\n",
        "        \"\"\"initializes learned embedding\n",
        "\n",
        "        Args:\n",
        "            same as __init__\n",
        "\n",
        "        Returns:\n",
        "            torch.float: initialized using original schemes\n",
        "        \"\"\"\n",
        "        if initialize_from_vocab:\n",
        "            # this takes first n_tokens words from vocab and uses as init of learnt embeddings\n",
        "            return self.wte.weight[:n_tokens].clone().detach()\n",
        "        # .half() is needed for Phi2\n",
        "        return torch.FloatTensor(n_tokens, wte.weight.size(1)).uniform_(-random_range, random_range).half().to('cuda')\n",
        "\n",
        "    def forward(self, tokens):\n",
        "        \"\"\"run forward pass\n",
        "\n",
        "        Args:\n",
        "            tokens (torch.long): input tokens before encoding\n",
        "\n",
        "        Returns:\n",
        "            torch.float: encoding of text concatenated with learned task specifc embedding\n",
        "        \"\"\"\n",
        "        # below line means that first n_tokens tokens will be ignored (?)\n",
        "        input_embedding = self.wte(tokens[:, self.n_tokens:])\n",
        "        learned_embedding = self.learned_embedding.repeat(input_embedding.size(0), 1, 1)\n",
        "        return torch.cat([learned_embedding, input_embedding], 1)"
      ],
      "metadata": {
        "id": "CZhoLYhJCE0D"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "EPcUhdnWB5cC"
      },
      "outputs": [],
      "source": [
        "# How many soft prompt tokens do we want to use.\n",
        "num_soft_prompt_tokens = 20\n",
        "initialize_from_vocab = False  # True"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.get_input_embeddings()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UQxAfk5b5L7i",
        "outputId": "03d27fc6-7eab-41f3-87e3-b4ef8136598b"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SoftEmbedding(\n",
              "  (wte): Embedding(51200, 2560)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N-SdU4762jY-",
        "outputId": "acb6f8aa-d9a1-4f09-9660-cdaf28057dc2"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CodeGenTokenizerFast(name_or_path='microsoft/phi-2', vocab_size=50257, model_max_length=2048, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
              "\t50256: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "\t50257: AddedToken(\"                               \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
              "\t50258: AddedToken(\"                              \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
              "\t50259: AddedToken(\"                             \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
              "\t50260: AddedToken(\"                            \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
              "\t50261: AddedToken(\"                           \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
              "\t50262: AddedToken(\"                          \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
              "\t50263: AddedToken(\"                         \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
              "\t50264: AddedToken(\"                        \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
              "\t50265: AddedToken(\"                       \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
              "\t50266: AddedToken(\"                      \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
              "\t50267: AddedToken(\"                     \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
              "\t50268: AddedToken(\"                    \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
              "\t50269: AddedToken(\"                   \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
              "\t50270: AddedToken(\"                  \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
              "\t50271: AddedToken(\"                 \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
              "\t50272: AddedToken(\"                \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
              "\t50273: AddedToken(\"               \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
              "\t50274: AddedToken(\"              \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
              "\t50275: AddedToken(\"             \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
              "\t50276: AddedToken(\"            \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
              "\t50277: AddedToken(\"           \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
              "\t50278: AddedToken(\"          \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
              "\t50279: AddedToken(\"         \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
              "\t50280: AddedToken(\"        \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
              "\t50281: AddedToken(\"       \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
              "\t50282: AddedToken(\"      \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
              "\t50283: AddedToken(\"     \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
              "\t50284: AddedToken(\"    \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
              "\t50285: AddedToken(\"   \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
              "\t50286: AddedToken(\"  \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
              "\t50287: AddedToken(\"\t\t\t\t\t\t\t\t\t\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
              "\t50288: AddedToken(\"\t\t\t\t\t\t\t\t\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
              "\t50289: AddedToken(\"\t\t\t\t\t\t\t\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
              "\t50290: AddedToken(\"\t\t\t\t\t\t\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
              "\t50291: AddedToken(\"\t\t\t\t\t\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
              "\t50292: AddedToken(\"\t\t\t\t\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
              "\t50293: AddedToken(\"\t\t\t\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
              "\t50294: AddedToken(\"\t\t\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
              "}"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "0t-TQM4jB5cC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "outputId": "3010dcb4-8b6d-46a7-e6c1-a6fbf36c8cdf"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'SoftEmbedding' object has no attribute 'weight'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-46-8371d5252cc3>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m s_wte = SoftEmbedding(model.get_input_embeddings(),\n\u001b[0m\u001b[1;32m      2\u001b[0m                       \u001b[0mn_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnum_soft_prompt_tokens\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                       initialize_from_vocab = initialize_from_vocab)\n",
            "\u001b[0;32m<ipython-input-42-6a16ab726abe>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, wte, n_tokens, random_range, initialize_from_vocab)\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwte\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwte\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mn_tokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         self.learned_embedding = nn.parameter.Parameter(self.initialize_embedding(wte,\n\u001b[0m\u001b[1;32m     19\u001b[0m                                                                                \u001b[0mn_tokens\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m                                                                                \u001b[0mrandom_range\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-42-6a16ab726abe>\u001b[0m in \u001b[0;36minitialize_embedding\u001b[0;34m(self, wte, n_tokens, random_range, initialize_from_vocab)\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwte\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mn_tokens\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0;31m# .half() is needed for Phi2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_tokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwte\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muniform_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mrandom_range\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_range\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhalf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cuda'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1686\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1687\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1688\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"'{type(self).__name__}' object has no attribute '{name}'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1690\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Module'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'SoftEmbedding' object has no attribute 'weight'"
          ]
        }
      ],
      "source": [
        "s_wte = SoftEmbedding(model.get_input_embeddings(),\n",
        "                      n_tokens = num_soft_prompt_tokens,\n",
        "                      initialize_from_vocab = initialize_from_vocab)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "s_wte"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0xtCVD-U2qzW",
        "outputId": "aa082f1d-3a56-46b4-a6ba-39bebf268c42"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SoftEmbedding(\n",
              "  (wte): Embedding(51200, 2560)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "JTL4XET5B5cC"
      },
      "outputs": [],
      "source": [
        "model.set_input_embeddings(s_wte)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "a202U5jLbU0n"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "yzc9vtg3B5cC"
      },
      "outputs": [],
      "source": [
        "def prepend_with_soft_prompts_padding(inputs, num_soft_tokens, pad_token_id = tokenizer.unk_token_id, labels = None):\n",
        "    \"\"\"\n",
        "    Need to pad attention_mask and input_ids to be full seq_len + n_learned_tokens,\n",
        "    even though it does not matter what you pad input_ids with, it's just to make HF happy.\n",
        "    More exp: the SoftEmbedding implementation ignores first num_soft_prompt_tokens of input tokens so this padding is to insert them at the beginning (and also make consistent with attention_mask length)\n",
        "    Padding is made of repeated \"unk_token\" (but it doesn't matter as it's ignored).\n",
        "    \"\"\"\n",
        "    batch_size = inputs['input_ids'].size(0)\n",
        "\n",
        "    inputs['input_ids'] = torch.cat([torch.full((batch_size, num_soft_tokens), pad_token_id), inputs['input_ids']], 1)\n",
        "    inputs['attention_mask'] = torch.cat([torch.full((batch_size, num_soft_tokens), 1), inputs['attention_mask']], 1)\n",
        "\n",
        "    if labels is None:\n",
        "        return inputs\n",
        "    else:\n",
        "        labels = torch.cat([torch.full((batch_size, num_soft_tokens), pad_token_id), labels], 1)\n",
        "        return inputs, labels\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inference"
      ],
      "metadata": {
        "id": "rjXwnhSPbX9g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DLwC7ejE2sbv",
        "outputId": "a87c1ea1-d738-4ae9-cbaa-04e199a48f32"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PhiForCausalLM(\n",
              "  (model): PhiModel(\n",
              "    (embed_tokens): SoftEmbedding(\n",
              "      (wte): Embedding(51200, 2560)\n",
              "    )\n",
              "    (embed_dropout): Dropout(p=0.0, inplace=False)\n",
              "    (layers): ModuleList(\n",
              "      (0-31): 32 x PhiDecoderLayer(\n",
              "        (self_attn): PhiAttention(\n",
              "          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
              "          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
              "          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
              "          (dense): Linear(in_features=2560, out_features=2560, bias=True)\n",
              "          (rotary_emb): PhiRotaryEmbedding()\n",
              "        )\n",
              "        (mlp): PhiMLP(\n",
              "          (activation_fn): NewGELUActivation()\n",
              "          (fc1): Linear(in_features=2560, out_features=10240, bias=True)\n",
              "          (fc2): Linear(in_features=10240, out_features=2560, bias=True)\n",
              "        )\n",
              "        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
              "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "    )\n",
              "    (final_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (lm_head): Linear(in_features=2560, out_features=51200, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = tokenizer(\"The capital of Australia is\", return_tensors=\"pt\")\n",
        "# inputs = create_empty_input()"
      ],
      "metadata": {
        "id": "4F8-pUf5Wqii"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8lZGnzHsW3S5",
        "outputId": "a77d60d7-0799-4ebe-bc0a-b0bacfacbbf2"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': tensor([[ 464, 3139,  286, 4505,  318]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1]], device='cuda:0')}"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.decode(inputs['input_ids'].squeeze(), skip_special_tokens=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "y7yLnfQ436M-",
        "outputId": "a511b785-fff5-47b0-cee3-7d0829a04da6"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The capital of Australia is'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = prepend_with_soft_prompts_padding(inputs, num_soft_prompt_tokens)\n",
        "\n",
        "print(inputs)\n",
        "print(tokenizer.decode(inputs['input_ids'].squeeze(), skip_special_tokens=False))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NVIVIsGL6cd9",
        "outputId": "e5ba75d4-557b-4aa6-a884-cbf20a9abdb6"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input_ids': tensor([[50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "           464,  3139,   286,  4505,   318]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1]], device='cuda:0')}\n",
            "<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>The capital of Australia is\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# outputs = model(**inputs)\n",
        "\n",
        "new_out_tokens = 10\n",
        "curr_inputs = inputs\n",
        "\n",
        "new_token_id = 0\n",
        "outputs = torch.cat([inputs['input_ids'], torch.full((1, new_out_tokens), 0) ], 1)\n",
        "\n",
        "\n",
        "model.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "  for i in range(new_out_tokens):\n",
        "\n",
        "    # outputs = model.generate(**inputs, max_length = curr_inputs['input_ids'].size(1) + 1)\n",
        "    raw_outputs = model(**curr_inputs)\n",
        "    # print(raw_outputs.logits.shape)\n",
        "\n",
        "    # new_token_id = outputs.squeeze()[-1]\n",
        "    new_token_id = raw_outputs.logits[:,-1,:].argmax(axis=-1).item()\n",
        "    outputs[:, (-new_out_tokens+i)] = new_token_id\n",
        "    # print(outputs)\n",
        "\n",
        "    # add the new token to inputs and repeat\n",
        "    curr_inputs['input_ids'] = torch.cat([curr_inputs['input_ids'], torch.full((1, 1), new_token_id)], 1)\n",
        "    curr_inputs['attention_mask'] = torch.cat([curr_inputs['attention_mask'], torch.full((1,1), 1)], 1)\n",
        "\n"
      ],
      "metadata": {
        "id": "s9zyLRpvWsRC"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(outputs.logits.shape)\n",
        "print(outputs)\n",
        "\n",
        "predicted_token_ids = outputs.squeeze()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CPuLynsUXb5p",
        "outputId": "ba57017b-1888-47b2-c5bc-b130a7e6d1c3"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "           464,  3139,   286,  4505,   318, 43296,    13,   198,    32,  1585,\n",
            "         23556,   531,    11,   366,   464]], device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = tokenizer.decode(predicted_token_ids, skip_special_tokens=True) #[0]\n",
        "\n",
        "# Print the decoded text\n",
        "print(f\"|{text}|\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S5q_PWwgCPPI",
        "outputId": "cf46d592-4a32-47a8-b31e-47897c2290c5"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "|The capital of Australia is Honolulu.\n",
            "Aristotle said, \"The|\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training"
      ],
      "metadata": {
        "id": "iGJix7h3ajG5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import copy\n",
        "\n",
        "# this token is ignored in loss - used to mask remainder of output\n",
        "ignored_token_id = tokenizer.unk_token_id\n",
        "\n",
        "target = \"The capital of Australia is Honolulu.\"\n",
        "\n",
        "target_tokens = tokenizer(target, return_tensors=\"pt\")\n",
        "target_len = target_tokens['input_ids'].size(1)\n",
        "\n",
        "print(target_tokens['input_ids'])\n",
        "\n",
        "# create the batch by repeating tokens, then in the loop mask endings\n",
        "\n",
        "target_tokens['input_ids'] = target_tokens['input_ids'].repeat(target_len - 1, 1)\n",
        "target_tokens['attention_mask'] = target_tokens['attention_mask'].repeat(target_len - 1, 1)\n",
        "\n",
        "# labels will be the next token, so clone and left-shift 1 hop\n",
        "labels = target_tokens['input_ids'].clone()\n",
        "labels = labels.roll(-1, dims=-1)\n",
        "\n",
        "# add masks\n",
        "for i in range(target_len - 1):\n",
        "  # pad right of i\n",
        "  #labels.append(target_tokens['input_ids'][i, i+1].item())\n",
        "  target_tokens['input_ids'][i, i+1:] = torch.full((1, target_len - i - 1), ignored_token_id)\n",
        "  labels[i, i+1:] = torch.full((1, target_len - i - 1), ignored_token_id)\n",
        "  target_tokens['attention_mask'][i, i+1:] = torch.full((1, target_len - i - 1), 0)\n",
        "\n",
        "# last token will never be fed as input so trim all tensors\n",
        "target_tokens['input_ids'] = target_tokens['input_ids'][:, :-1]\n",
        "target_tokens['attention_mask'] = target_tokens['attention_mask'][:, :-1]\n",
        "labels = labels[:, :-1]\n",
        "\n",
        "# Finally, pad inputs with soft prompts\n",
        "target_tokens, labels = prepend_with_soft_prompts_padding(target_tokens, num_soft_prompt_tokens, labels = labels)\n",
        "\n",
        "\n",
        "print(target_tokens['input_ids'])\n",
        "print(target_tokens['attention_mask'])\n",
        "print(labels)\n"
      ],
      "metadata": {
        "id": "47rAABJKCTBo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "50098ce6-4612-413e-b174-fe064f0e9e77"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[  464,  3139,   286,  4505,   318, 43296,    13]], device='cuda:0')\n",
            "tensor([[50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "           464, 50256, 50256, 50256, 50256, 50256],\n",
            "        [50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "           464,  3139, 50256, 50256, 50256, 50256],\n",
            "        [50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "           464,  3139,   286, 50256, 50256, 50256],\n",
            "        [50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "           464,  3139,   286,  4505, 50256, 50256],\n",
            "        [50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "           464,  3139,   286,  4505,   318, 50256],\n",
            "        [50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "           464,  3139,   286,  4505,   318, 43296]], device='cuda:0')\n",
            "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
            "         0, 0],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
            "         0, 0],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
            "         0, 0],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         0, 0],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 0],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1]], device='cuda:0')\n",
            "tensor([[50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "          3139, 50256, 50256, 50256, 50256, 50256],\n",
            "        [50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "          3139,   286, 50256, 50256, 50256, 50256],\n",
            "        [50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "          3139,   286,  4505, 50256, 50256, 50256],\n",
            "        [50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "          3139,   286,  4505,   318, 50256, 50256],\n",
            "        [50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "          3139,   286,  4505,   318, 43296, 50256],\n",
            "        [50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "          3139,   286,  4505,   318, 43296,    13]], device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# freeze entire model, then unfreeze soft embeddings\n",
        "model.requires_grad_(False)\n",
        "s_wte.requires_grad_(True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-BzGQohtaO4s",
        "outputId": "0d9e6171-f75e-4b15-d6e4-59f32c053505"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SoftEmbedding(\n",
              "  (wte): Embedding(51200, 2560)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_loss(criterion, logits, labels):\n",
        "\n",
        "    logits_flat = logits.view(-1, logits.size(-1))\n",
        "    # print(logits_flat.shape)\n",
        "\n",
        "    labels_flat = labels.flatten()\n",
        "    # print(labels_flat.shape)\n",
        "\n",
        "    loss = criterion(logits_flat, labels_flat)\n",
        "\n",
        "    loss_per_batch = loss.mean()\n",
        "\n",
        "    return loss_per_batch"
      ],
      "metadata": {
        "id": "8JSc45v0UonR"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop.\n",
        "# Note we don't need/want an eval set - unlike in typical training, we do want to overfit to train data as much as possible.\n",
        "\n",
        "model.train()\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(ignore_index = ignored_token_id, reduction='none')\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01) # TODO: Try Adam\n",
        "\n",
        "best_loss = 1e9\n",
        "best_soft_prompts = None\n",
        "\n",
        "# Train the model\n",
        "num_epochs = 1000\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    # Forward pass\n",
        "    outputs = model(input_ids = target_tokens['input_ids'], attention_mask = target_tokens['attention_mask'])\n",
        "\n",
        "    # print(outputs.logits.shape) #, outputs.logits)\n",
        "    # print(labels.shape, labels)\n",
        "\n",
        "    loss = compute_loss(criterion, outputs.logits, labels)\n",
        "\n",
        "    # Backpropagation\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # Save best\n",
        "    if loss.item() < best_loss:\n",
        "        best_loss = loss.item()\n",
        "        best_soft_prompts = copy.deepcopy(s_wte) # .clone() fails :(\n",
        "        print('--- NEW BEST: Epoch: {}, Loss: {:.4f}'.format(epoch+1, best_loss))\n",
        "\n",
        "\n",
        "    # Print the loss\n",
        "    if epoch % 10 == 0:\n",
        "        print('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, loss.item()))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RF8Y3i5hcEji",
        "outputId": "143910b6-4147-421d-f629-9c7a7a77c5fb"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- NEW BEST: Epoch: 1, Loss: 0.7091\n",
            "Epoch [1/1000], Loss: 0.7091\n",
            "--- NEW BEST: Epoch: 2, Loss: 0.6532\n",
            "--- NEW BEST: Epoch: 7, Loss: 0.6437\n",
            "--- NEW BEST: Epoch: 9, Loss: 0.6272\n",
            "Epoch [11/1000], Loss: 0.6626\n",
            "--- NEW BEST: Epoch: 17, Loss: 0.6266\n",
            "Epoch [21/1000], Loss: 0.6369\n",
            "--- NEW BEST: Epoch: 23, Loss: 0.6184\n",
            "--- NEW BEST: Epoch: 25, Loss: 0.6179\n",
            "--- NEW BEST: Epoch: 26, Loss: 0.6011\n",
            "--- NEW BEST: Epoch: 31, Loss: 0.5834\n",
            "Epoch [31/1000], Loss: 0.5834\n",
            "Epoch [41/1000], Loss: 0.5905\n",
            "--- NEW BEST: Epoch: 42, Loss: 0.5626\n",
            "--- NEW BEST: Epoch: 51, Loss: 0.5397\n",
            "Epoch [51/1000], Loss: 0.5397\n",
            "--- NEW BEST: Epoch: 56, Loss: 0.5113\n",
            "Epoch [61/1000], Loss: 0.5437\n",
            "--- NEW BEST: Epoch: 71, Loss: 0.5093\n",
            "Epoch [71/1000], Loss: 0.5093\n",
            "--- NEW BEST: Epoch: 72, Loss: 0.5053\n",
            "--- NEW BEST: Epoch: 78, Loss: 0.5028\n",
            "--- NEW BEST: Epoch: 80, Loss: 0.4951\n",
            "Epoch [81/1000], Loss: 0.5114\n",
            "--- NEW BEST: Epoch: 89, Loss: 0.4835\n",
            "Epoch [91/1000], Loss: 0.4921\n",
            "Epoch [101/1000], Loss: 0.5043\n",
            "--- NEW BEST: Epoch: 102, Loss: 0.4768\n",
            "--- NEW BEST: Epoch: 103, Loss: 0.4697\n",
            "--- NEW BEST: Epoch: 107, Loss: 0.4644\n",
            "Epoch [111/1000], Loss: 0.4807\n",
            "--- NEW BEST: Epoch: 114, Loss: 0.4499\n",
            "Epoch [121/1000], Loss: 0.4675\n",
            "Epoch [131/1000], Loss: 0.4667\n",
            "--- NEW BEST: Epoch: 135, Loss: 0.4439\n",
            "--- NEW BEST: Epoch: 136, Loss: 0.4381\n",
            "Epoch [141/1000], Loss: 0.4587\n",
            "--- NEW BEST: Epoch: 143, Loss: 0.4337\n",
            "--- NEW BEST: Epoch: 150, Loss: 0.4236\n",
            "Epoch [151/1000], Loss: 0.4464\n",
            "Epoch [161/1000], Loss: 0.4262\n",
            "--- NEW BEST: Epoch: 162, Loss: 0.4204\n",
            "--- NEW BEST: Epoch: 164, Loss: 0.4180\n",
            "--- NEW BEST: Epoch: 169, Loss: 0.4110\n",
            "Epoch [171/1000], Loss: 0.4344\n",
            "--- NEW BEST: Epoch: 174, Loss: 0.4081\n",
            "--- NEW BEST: Epoch: 181, Loss: 0.4041\n",
            "Epoch [181/1000], Loss: 0.4041\n",
            "--- NEW BEST: Epoch: 191, Loss: 0.4004\n",
            "Epoch [191/1000], Loss: 0.4004\n",
            "--- NEW BEST: Epoch: 192, Loss: 0.3898\n",
            "--- NEW BEST: Epoch: 198, Loss: 0.3896\n",
            "--- NEW BEST: Epoch: 199, Loss: 0.3799\n",
            "Epoch [201/1000], Loss: 0.3928\n",
            "--- NEW BEST: Epoch: 203, Loss: 0.3775\n",
            "--- NEW BEST: Epoch: 204, Loss: 0.3756\n",
            "--- NEW BEST: Epoch: 207, Loss: 0.3713\n",
            "Epoch [211/1000], Loss: 0.3727\n",
            "--- NEW BEST: Epoch: 215, Loss: 0.3672\n",
            "--- NEW BEST: Epoch: 218, Loss: 0.3576\n",
            "--- NEW BEST: Epoch: 220, Loss: 0.3562\n",
            "Epoch [221/1000], Loss: 0.3661\n",
            "--- NEW BEST: Epoch: 226, Loss: 0.3552\n",
            "Epoch [231/1000], Loss: 0.3673\n",
            "--- NEW BEST: Epoch: 232, Loss: 0.3384\n",
            "Epoch [241/1000], Loss: 0.3488\n",
            "--- NEW BEST: Epoch: 244, Loss: 0.3277\n",
            "Epoch [251/1000], Loss: 0.3362\n",
            "--- NEW BEST: Epoch: 259, Loss: 0.3222\n",
            "--- NEW BEST: Epoch: 260, Loss: 0.3154\n",
            "--- NEW BEST: Epoch: 261, Loss: 0.3068\n",
            "Epoch [261/1000], Loss: 0.3068\n",
            "--- NEW BEST: Epoch: 263, Loss: 0.3048\n",
            "--- NEW BEST: Epoch: 268, Loss: 0.2997\n",
            "--- NEW BEST: Epoch: 270, Loss: 0.2966\n",
            "--- NEW BEST: Epoch: 271, Loss: 0.2947\n",
            "Epoch [271/1000], Loss: 0.2947\n",
            "--- NEW BEST: Epoch: 279, Loss: 0.2878\n",
            "--- NEW BEST: Epoch: 280, Loss: 0.2759\n",
            "Epoch [281/1000], Loss: 0.3078\n",
            "--- NEW BEST: Epoch: 287, Loss: 0.2566\n",
            "Epoch [291/1000], Loss: 0.2785\n",
            "--- NEW BEST: Epoch: 293, Loss: 0.2555\n",
            "--- NEW BEST: Epoch: 297, Loss: 0.2425\n",
            "Epoch [301/1000], Loss: 0.2883\n",
            "--- NEW BEST: Epoch: 302, Loss: 0.2214\n",
            "Epoch [311/1000], Loss: 0.2280\n",
            "--- NEW BEST: Epoch: 314, Loss: 0.2202\n",
            "--- NEW BEST: Epoch: 319, Loss: 0.2058\n",
            "Epoch [321/1000], Loss: 0.2526\n",
            "--- NEW BEST: Epoch: 324, Loss: 0.2028\n",
            "--- NEW BEST: Epoch: 330, Loss: 0.2022\n",
            "--- NEW BEST: Epoch: 331, Loss: 0.1953\n",
            "Epoch [331/1000], Loss: 0.1953\n",
            "--- NEW BEST: Epoch: 333, Loss: 0.1944\n",
            "--- NEW BEST: Epoch: 334, Loss: 0.1919\n",
            "--- NEW BEST: Epoch: 335, Loss: 0.1873\n",
            "Epoch [341/1000], Loss: 0.1934\n",
            "--- NEW BEST: Epoch: 343, Loss: 0.1834\n",
            "--- NEW BEST: Epoch: 344, Loss: 0.1703\n",
            "Epoch [351/1000], Loss: 0.1885\n",
            "--- NEW BEST: Epoch: 352, Loss: 0.1557\n",
            "--- NEW BEST: Epoch: 355, Loss: 0.1517\n",
            "--- NEW BEST: Epoch: 360, Loss: 0.1372\n",
            "Epoch [361/1000], Loss: 0.1445\n",
            "Epoch [371/1000], Loss: 0.1491\n",
            "--- NEW BEST: Epoch: 376, Loss: 0.1364\n",
            "--- NEW BEST: Epoch: 379, Loss: 0.1332\n",
            "--- NEW BEST: Epoch: 381, Loss: 0.1163\n",
            "Epoch [381/1000], Loss: 0.1163\n",
            "Epoch [391/1000], Loss: 0.2082\n",
            "--- NEW BEST: Epoch: 392, Loss: 0.1060\n",
            "--- NEW BEST: Epoch: 400, Loss: 0.0897\n",
            "Epoch [401/1000], Loss: 0.1157\n",
            "Epoch [411/1000], Loss: 0.1372\n",
            "--- NEW BEST: Epoch: 420, Loss: 0.0867\n",
            "Epoch [421/1000], Loss: 0.1015\n",
            "--- NEW BEST: Epoch: 423, Loss: 0.0821\n",
            "--- NEW BEST: Epoch: 428, Loss: 0.0790\n",
            "--- NEW BEST: Epoch: 431, Loss: 0.0745\n",
            "Epoch [431/1000], Loss: 0.0745\n",
            "Epoch [441/1000], Loss: 0.1085\n",
            "--- NEW BEST: Epoch: 446, Loss: 0.0605\n",
            "Epoch [451/1000], Loss: 0.0708\n",
            "Epoch [461/1000], Loss: 0.0703\n",
            "--- NEW BEST: Epoch: 468, Loss: 0.0570\n",
            "Epoch [471/1000], Loss: 0.0988\n",
            "--- NEW BEST: Epoch: 476, Loss: 0.0440\n",
            "Epoch [481/1000], Loss: 0.0637\n",
            "Epoch [491/1000], Loss: 0.0582\n",
            "--- NEW BEST: Epoch: 494, Loss: 0.0428\n",
            "Epoch [501/1000], Loss: 0.0490\n",
            "--- NEW BEST: Epoch: 506, Loss: 0.0425\n",
            "--- NEW BEST: Epoch: 510, Loss: 0.0374\n",
            "Epoch [511/1000], Loss: 0.0460\n",
            "--- NEW BEST: Epoch: 516, Loss: 0.0226\n",
            "Epoch [521/1000], Loss: 0.0645\n",
            "Epoch [531/1000], Loss: 0.0363\n",
            "Epoch [541/1000], Loss: 0.0374\n",
            "Epoch [551/1000], Loss: 0.0610\n",
            "Epoch [561/1000], Loss: 0.0325\n",
            "--- NEW BEST: Epoch: 569, Loss: 0.0222\n",
            "Epoch [571/1000], Loss: 0.0342\n",
            "Epoch [581/1000], Loss: 0.0282\n",
            "Epoch [591/1000], Loss: 0.0292\n",
            "Epoch [601/1000], Loss: 0.0410\n",
            "--- NEW BEST: Epoch: 605, Loss: 0.0181\n",
            "Epoch [611/1000], Loss: 0.0261\n",
            "Epoch [621/1000], Loss: 0.0280\n",
            "--- NEW BEST: Epoch: 630, Loss: 0.0133\n",
            "Epoch [631/1000], Loss: 0.0276\n",
            "--- NEW BEST: Epoch: 640, Loss: 0.0130\n",
            "Epoch [641/1000], Loss: 0.0188\n",
            "Epoch [651/1000], Loss: 0.0545\n",
            "Epoch [661/1000], Loss: 0.0621\n",
            "--- NEW BEST: Epoch: 669, Loss: 0.0097\n",
            "Epoch [671/1000], Loss: 0.0298\n",
            "Epoch [681/1000], Loss: 0.0121\n",
            "Epoch [691/1000], Loss: 0.0296\n",
            "--- NEW BEST: Epoch: 695, Loss: 0.0090\n",
            "Epoch [701/1000], Loss: 0.0157\n",
            "Epoch [711/1000], Loss: 0.0189\n",
            "Epoch [721/1000], Loss: 0.0401\n",
            "--- NEW BEST: Epoch: 726, Loss: 0.0083\n",
            "Epoch [731/1000], Loss: 0.0094\n",
            "Epoch [741/1000], Loss: 0.0207\n",
            "--- NEW BEST: Epoch: 749, Loss: 0.0080\n",
            "Epoch [751/1000], Loss: 0.0198\n",
            "--- NEW BEST: Epoch: 758, Loss: 0.0079\n",
            "Epoch [761/1000], Loss: 0.0143\n",
            "--- NEW BEST: Epoch: 769, Loss: 0.0066\n",
            "Epoch [771/1000], Loss: 0.0097\n",
            "Epoch [781/1000], Loss: 0.0225\n",
            "Epoch [791/1000], Loss: 0.0230\n",
            "Epoch [801/1000], Loss: 0.0134\n",
            "Epoch [811/1000], Loss: 0.0155\n",
            "--- NEW BEST: Epoch: 816, Loss: 0.0041\n",
            "Epoch [821/1000], Loss: 0.0226\n",
            "Epoch [831/1000], Loss: 0.0107\n",
            "Epoch [841/1000], Loss: 0.0075\n",
            "Epoch [851/1000], Loss: 0.0076\n",
            "--- NEW BEST: Epoch: 857, Loss: 0.0036\n",
            "Epoch [861/1000], Loss: 0.0201\n",
            "Epoch [871/1000], Loss: 0.0176\n",
            "Epoch [881/1000], Loss: 0.0049\n",
            "Epoch [891/1000], Loss: 0.0132\n",
            "Epoch [901/1000], Loss: 0.0117\n",
            "Epoch [911/1000], Loss: 0.0079\n",
            "Epoch [921/1000], Loss: 0.0115\n",
            "Epoch [931/1000], Loss: 0.0090\n",
            "Epoch [941/1000], Loss: 0.0078\n",
            "Epoch [951/1000], Loss: 0.0057\n",
            "Epoch [961/1000], Loss: 0.0238\n",
            "Epoch [971/1000], Loss: 0.0066\n",
            "Epoch [981/1000], Loss: 0.0042\n",
            "Epoch [991/1000], Loss: 0.0297\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "best_loss"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7vSrhLtc3feX",
        "outputId": "82d8bd0c-b96c-48f4-d3ce-ee2290db5ec7"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0036146200727671385"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "best_loss"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SHkMP9eu28yt",
        "outputId": "b307e09d-062b-49e4-d28c-2a0a6bb6313c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.006296176929026842"
            ]
          },
          "metadata": {},
          "execution_count": 127
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "best_soft_prompts"
      ],
      "metadata": {
        "id": "r3S2rjr0hb19",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2305cd1d-05fd-409f-a06c-b6e5191024d3"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SoftEmbedding(\n",
              "  (wte): Embedding(51200, 2560)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the best soft prompts on the model.\n",
        "model.set_input_embeddings(best_soft_prompts)"
      ],
      "metadata": {
        "id": "oXti4K4HrEqu"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"===============================================================================================\")\n",
        "print(\">>>>> Now go back to Inference section and see what you get with trained soft prompts =] <<<<<<\")\n",
        "print(\"===============================================================================================\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xVcssWH5XZ2S",
        "outputId": "6f971d31-f0b9-425e-dc35-74c1eb824b59"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "===============================================================================================\n",
            ">>>>> Now go back to Inference section and see what you get with trained soft prompts =] <<<<<<\n",
            "===============================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Y3tnYi9MXaOz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# USING HF LIBRARY\n",
        "# from transformers import TrainingArguments, Trainer\n",
        "\n",
        "# training_args = TrainingArguments(\n",
        "#     output_dir=\"./model_checkpoints\",  # Output directory for checkpoints\n",
        "#     num_train_epochs=3,  # Total number of training epochs\n",
        "#     per_device_train_batch_size=16,  # Batch size per device\n",
        "#     per_device_eval_batch_size=16,  # Batch size for evaluation\n",
        "#     warmup_steps=500,  # Number of warmup steps\n",
        "#     logging_steps=100,  # Number of steps between logging\n",
        "#     save_steps=1000,  # Number of steps between saving checkpoints\n",
        "#     evaluation_strategy=\"steps\",  # Evaluation strategy\n",
        "#     eval_steps=1000,  # Number of steps between evaluations\n",
        "# )\n",
        "\n",
        "# trainer = Trainer(\n",
        "#     model=model,  # The model to train\n",
        "#     args=training_args,  # Training arguments\n",
        "#     train_dataset=train_dataset,  # Training dataset\n",
        "#     eval_dataset=eval_dataset,  # Evaluation dataset\n",
        "# )\n",
        "\n",
        "# trainer.train()"
      ],
      "metadata": {
        "id": "-yhD7Et_XQmD"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "08f5a8ed158846d999b8e647f4bad9b5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_775a3884c9f6484db8bcb5fe2d366e6d",
              "IPY_MODEL_38e1ea780b4e4753be04d997ade161f3",
              "IPY_MODEL_29e2d02507d144ec8b2854997f0b75c5"
            ],
            "layout": "IPY_MODEL_567d22ec0e904a7cb80f22544d3be080"
          }
        },
        "775a3884c9f6484db8bcb5fe2d366e6d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8d4d3f45454c4adfa05553359cce76d4",
            "placeholder": "",
            "style": "IPY_MODEL_96cb5d61eaa74e6db643b98d7c538741",
            "value": "Loadingcheckpointshards:100%"
          }
        },
        "38e1ea780b4e4753be04d997ade161f3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1be8877023634452bf640217bbbd6fe1",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1458f0e25041457c9c5bd483d43605d3",
            "value": 2
          }
        },
        "29e2d02507d144ec8b2854997f0b75c5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d54c465a216343fcbf64409b940a02a0",
            "placeholder": "",
            "style": "IPY_MODEL_17872042afd74463ad5f6ae4b1cf2c5e",
            "value": "2/2[00:16&lt;00:00,6.83s/it]"
          }
        },
        "567d22ec0e904a7cb80f22544d3be080": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8d4d3f45454c4adfa05553359cce76d4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "96cb5d61eaa74e6db643b98d7c538741": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1be8877023634452bf640217bbbd6fe1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1458f0e25041457c9c5bd483d43605d3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d54c465a216343fcbf64409b940a02a0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "17872042afd74463ad5f6ae4b1cf2c5e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}