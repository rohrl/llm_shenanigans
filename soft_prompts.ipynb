{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rohrl/llm_shenanigans/blob/main/soft_prompts.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# based on https://github.com/kipgparker/soft-prompt-tuning/blob/main/example.ipynb"
      ],
      "metadata": {
        "id": "eEHnZz7DCAcT"
      },
      "execution_count": 181,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 182,
      "metadata": {
        "id": "ZTriOPpNB5cB"
      },
      "outputs": [],
      "source": [
        "from transformers import GPT2LMHeadModel, GPT2TokenizerFast"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 183,
      "metadata": {
        "id": "Ur3z7dliB5cC"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class SoftEmbedding(nn.Module):\n",
        "    def __init__(self,\n",
        "                wte: nn.Embedding,\n",
        "                n_tokens: int = 10,\n",
        "                random_range: float = 0.5,\n",
        "                initialize_from_vocab: bool = True):\n",
        "        \"\"\"appends learned embedding to\n",
        "\n",
        "        Args:\n",
        "            wte (nn.Embedding): original transformer word embedding\n",
        "            n_tokens (int, optional): number of tokens for task. Defaults to 10.\n",
        "            random_range (float, optional): range to init embedding (if not initialize from vocab). Defaults to 0.5.\n",
        "            initialize_from_vocab (bool, optional): initalizes from default vocab. Defaults to True.\n",
        "        \"\"\"\n",
        "        super(SoftEmbedding, self).__init__()\n",
        "        self.wte = wte\n",
        "        self.n_tokens = n_tokens\n",
        "        self.learned_embedding = nn.parameter.Parameter(self.initialize_embedding(wte,\n",
        "                                                                               n_tokens,\n",
        "                                                                               random_range,\n",
        "                                                                               initialize_from_vocab))\n",
        "\n",
        "    def initialize_embedding(self,\n",
        "                             wte: nn.Embedding,\n",
        "                             n_tokens: int = 10,\n",
        "                             random_range: float = 0.5,\n",
        "                             initialize_from_vocab: bool = True):\n",
        "        \"\"\"initializes learned embedding\n",
        "\n",
        "        Args:\n",
        "            same as __init__\n",
        "\n",
        "        Returns:\n",
        "            torch.float: initialized using original schemes\n",
        "        \"\"\"\n",
        "        if initialize_from_vocab:\n",
        "            # this takes first n_tokens words from vocab and uses as init of learnt embeddings\n",
        "            return self.wte.weight[:n_tokens].clone().detach()\n",
        "        return torch.FloatTensor(n_tokens, wte.weight.size(1)).uniform_(-random_range, random_range)\n",
        "\n",
        "    def forward(self, tokens):\n",
        "        \"\"\"run forward pass\n",
        "\n",
        "        Args:\n",
        "            tokens (torch.long): input tokens before encoding\n",
        "\n",
        "        Returns:\n",
        "            torch.float: encoding of text concatenated with learned task specifc embedding\n",
        "        \"\"\"\n",
        "        # below line means that first n_tokens tokens will be ignored (?)\n",
        "        input_embedding = self.wte(tokens[:, self.n_tokens:])\n",
        "        learned_embedding = self.learned_embedding.repeat(input_embedding.size(0), 1, 1)\n",
        "        return torch.cat([learned_embedding, input_embedding], 1)"
      ],
      "metadata": {
        "id": "CZhoLYhJCE0D"
      },
      "execution_count": 184,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 185,
      "metadata": {
        "id": "EPcUhdnWB5cC"
      },
      "outputs": [],
      "source": [
        "n_tokens = 3 # 20\n",
        "initialize_from_vocab = False  # True"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UVK1UxLC5Lhj"
      },
      "execution_count": 185,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 186,
      "metadata": {
        "id": "XguFZWJ0B5cC"
      },
      "outputs": [],
      "source": [
        "tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n",
        "model = GPT2LMHeadModel.from_pretrained('gpt2')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.get_input_embeddings()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UQxAfk5b5L7i",
        "outputId": "e7d709e7-857a-4f42-9410-d32fa6675d9c"
      },
      "execution_count": 187,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Embedding(50257, 768)"
            ]
          },
          "metadata": {},
          "execution_count": 187
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N-SdU4762jY-",
        "outputId": "c6019327-0c89-426d-8aa8-998de22fd5a4"
      },
      "execution_count": 188,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GPT2TokenizerFast(name_or_path='gpt2', vocab_size=50257, model_max_length=1024, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
              "\t50256: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
              "}"
            ]
          },
          "metadata": {},
          "execution_count": 188
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 189,
      "metadata": {
        "id": "0t-TQM4jB5cC"
      },
      "outputs": [],
      "source": [
        "s_wte = SoftEmbedding(model.get_input_embeddings(),\n",
        "                      n_tokens=n_tokens,\n",
        "                      initialize_from_vocab=initialize_from_vocab)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "s_wte"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0xtCVD-U2qzW",
        "outputId": "8f820a12-c6d1-4b6a-b7e1-436492be681f"
      },
      "execution_count": 190,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SoftEmbedding(\n",
              "  (wte): Embedding(50257, 768)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 190
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 191,
      "metadata": {
        "id": "JTL4XET5B5cC"
      },
      "outputs": [],
      "source": [
        "model.set_input_embeddings(s_wte)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LlSpJmei2qP4"
      },
      "execution_count": 191,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = tokenizer(\"May the force be\", return_tensors=\"pt\")\n"
      ],
      "metadata": {
        "id": "4F8-pUf5Wqii"
      },
      "execution_count": 348,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8lZGnzHsW3S5",
        "outputId": "a1cba117-7d56-450c-8e33-68032d53f7cf"
      },
      "execution_count": 349,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': tensor([[6747,  262, 2700,  307]]), 'attention_mask': tensor([[1, 1, 1, 1]])}"
            ]
          },
          "metadata": {},
          "execution_count": 349
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.decode(inputs.input_ids.squeeze(), skip_special_tokens=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "y7yLnfQ436M-",
        "outputId": "d279a4a6-50a7-4895-87eb-20b9d540c9bb"
      },
      "execution_count": 350,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'May the force be'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 350
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 351,
      "metadata": {
        "id": "yzc9vtg3B5cC"
      },
      "outputs": [],
      "source": [
        "\n",
        "# need to pad attention_mask and input_ids to be full seq_len + n_learned_tokens\n",
        "# even though it does not matter what you pad input_ids with, it's just to make HF happy\n",
        "# more exp: the SoftEmbedding implementation ignores first n_tokens of input tokens so this padding is to insert them at the beginning (and also make consistent with attention_mask length)\n",
        "# Padding is made of repeated \"unk_token\" (but it doesn't matter as it's ignored)\n",
        "inputs['input_ids'] = torch.cat([torch.full((1,n_tokens), tokenizer.unk_token_id), inputs['input_ids']], 1)\n",
        "inputs['attention_mask'] = torch.cat([torch.full((1,n_tokens), 1), inputs['attention_mask']], 1)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(inputs)\n",
        "print(tokenizer.decode(inputs.input_ids.squeeze(), skip_special_tokens=False))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NVIVIsGL6cd9",
        "outputId": "809cef22-d4b7-4fcd-9fc1-57546e804eb2"
      },
      "execution_count": 352,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input_ids': tensor([[50256, 50256, 50256,  6747,   262,  2700,   307]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1]])}\n",
            "<|endoftext|><|endoftext|><|endoftext|>May the force be\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# outputs = model(**inputs)\n",
        "\n",
        "new_out_tokens = 10\n",
        "curr_inputs = inputs\n",
        "\n",
        "new_token_id = 0\n",
        "outputs = torch.cat([inputs.input_ids, torch.full((1, new_out_tokens), 0) ], 1)\n",
        "\n",
        "for i in range(new_out_tokens):\n",
        "\n",
        "  # outputs = model.generate(**inputs, max_length = curr_inputs.input_ids.size(1) + 1)\n",
        "  raw_outputs = model(**curr_inputs)\n",
        "  # print(raw_outputs.logits.shape)\n",
        "\n",
        "  # new_token_id = outputs.squeeze()[-1]\n",
        "  new_token_id = raw_outputs.logits[:,-1,:].argmax(axis=-1).item()\n",
        "  outputs[:, (-new_out_tokens+i)] = new_token_id\n",
        "  # print(outputs)\n",
        "\n",
        "  # add the new token to inputs and repeat\n",
        "  curr_inputs['input_ids'] = torch.cat([curr_inputs['input_ids'], torch.full((1, 1), new_token_id)], 1)\n",
        "  curr_inputs['attention_mask'] = torch.cat([curr_inputs['attention_mask'], torch.full((1,1), 1)], 1)\n",
        "\n"
      ],
      "metadata": {
        "id": "s9zyLRpvWsRC"
      },
      "execution_count": 353,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "prM4lP7vF_IH"
      },
      "execution_count": 353,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(outputs.logits.shape)\n",
        "print(outputs)\n",
        "\n",
        "predicted_token_ids = outputs.squeeze()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CPuLynsUXb5p",
        "outputId": "0fc99100-52ad-43a6-f152-5f8eb3b714c8"
      },
      "execution_count": 354,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[50256, 50256, 50256,  6747,   262,  2700,   307,   351,   345,    13,\n",
            "           198,   198,   464,   691,  1517,   326,   338]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = tokenizer.decode(predicted_token_ids, skip_special_tokens=False) #[0]\n",
        "\n",
        "# Print the decoded text\n",
        "print(f\"|{text}|\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S5q_PWwgCPPI",
        "outputId": "a8cfdb31-b6ea-4095-b2ba-52977ff439b8"
      },
      "execution_count": 355,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "|<|endoftext|><|endoftext|><|endoftext|>May the force be with you.\n",
            "\n",
            "The only thing that's|\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Jdqj2JwYCYXm"
      },
      "execution_count": 217,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "47rAABJKCTBo"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}