{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rohrl/llm_shenanigans/blob/main/soft_prompts.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# based on https://github.com/kipgparker/soft-prompt-tuning/blob/main/example.ipynb"
      ],
      "metadata": {
        "id": "eEHnZz7DCAcT"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentencepiece transformers accelerate einops"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Zde6dRjk8e0",
        "outputId": "be457520-c792-4d7e-b623-52c70f32a732"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (0.1.99)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.38.2)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.28.0)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (0.7.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.3)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.2)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.2.1+cu121)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.10.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.3)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ZTriOPpNB5cB"
      },
      "outputs": [],
      "source": [
        "from transformers import GPT2LMHeadModel, GPT2TokenizerFast, AutoModelForCausalLM, AutoTokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Ur3z7dliB5cC"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.set_default_device('cuda')"
      ],
      "metadata": {
        "id": "UwsFPl2TP5j0"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "XguFZWJ0B5cC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 171,
          "referenced_widgets": [
            "2bda861425e149d5bc1a9a3d02c4a912",
            "7b5c5495b2f746bb829a9910e8b5af23",
            "82403d009a8b4883a5ffd1ea86e76f79",
            "6776bb28d113464183499b2f3b51636d",
            "0b403370814e40108e9205e3d37fbd84",
            "a71b4a5909d946aa882bce7e5e9559cc",
            "ead16790a87b406fabc77c277754c4ac",
            "08da68d32c2d4a98a00039d467e6e59a",
            "29a9f31dc5684b7ab7be88727969fdad",
            "e875e3a90f7c4130a4c347cd5db39bcb",
            "9861695659994af89cfe86736629285a"
          ]
        },
        "outputId": "bf4cc61d-4315-4376-f8b5-29e75ab4f65b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2bda861425e149d5bc1a9a3d02c4a912"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "# tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n",
        "# model = GPT2LMHeadModel.from_pretrained('gpt2', device_map=\"cuda\")\n",
        "\n",
        "# model_id = \"itsliupeng/llama2_7b_mmlu\"\n",
        "# model = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"cuda\")\n",
        "# tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\"microsoft/phi-2\", torch_dtype=\"auto\", device_map=\"cuda\", trust_remote_code=True)\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/phi-2\", trust_remote_code=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HTLW-jE0hcjn",
        "outputId": "65e610b7-d047-4d5e-9401-fb536194ed84"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda', index=0)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sanity check"
      ],
      "metadata": {
        "id": "oLIP2Myzhd0q"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uNB7j7UJ6etf"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_empty_input():\n",
        "    return {'input_ids': torch.empty(size=(1,0)).to(torch.int64), 'attention_mask': torch.empty(size=(1,0)).to(torch.int64)}"
      ],
      "metadata": {
        "id": "Enmki7qE59tt"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sanity_text = \"The capital of Australia is\"\n",
        "sanity_output = model.generate(input_ids = tokenizer.encode(sanity_text, return_tensors=\"pt\"), max_length=15, num_return_sequences=1)\n",
        "print(\"*******************************************\\n\" +\n",
        "      tokenizer.decode(sanity_output[0], skip_special_tokens=True) +\n",
        "      \"\\n*******************************************\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lCDqd103hg3x",
        "outputId": "741f390a-fd48-462c-b863-4701fcc7cb76"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*******************************************\n",
            "The capital of Australia is Canberra.\n",
            "    How many more tickets should Is\n",
            "*******************************************\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# inputs = create_empty_input()\n",
        "# model.generate(inputs['input_ids'], max_length=10, num_return_sequences=1)\n",
        "# print(\"==================\\n\" + tokenizer.decode(sanity_output[0], skip_special_tokens=True) + \"\\n==================\\n\")"
      ],
      "metadata": {
        "id": "1dTAN8s76v8A"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "3_CV8MKh6d_x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Soft Embeddings"
      ],
      "metadata": {
        "id": "e-m2OeNuhh0T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SoftEmbedding(nn.Module):\n",
        "    def __init__(self,\n",
        "                wte: nn.Embedding,\n",
        "                n_tokens: int = 10,\n",
        "                random_range: float = 0.5,\n",
        "                initialize_from_vocab: bool = True):\n",
        "        \"\"\"appends learned embedding to\n",
        "\n",
        "        Args:\n",
        "            wte (nn.Embedding): original transformer word embedding\n",
        "            n_tokens (int, optional): number of tokens for task. Defaults to 10.\n",
        "            random_range (float, optional): range to init embedding (if not initialize from vocab). Defaults to 0.5.\n",
        "            initialize_from_vocab (bool, optional): initalizes from default vocab. Defaults to True.\n",
        "        \"\"\"\n",
        "        super(SoftEmbedding, self).__init__()\n",
        "        self.wte = wte\n",
        "        self.n_tokens = n_tokens\n",
        "        self.learned_embedding = nn.parameter.Parameter(self.initialize_embedding(wte,\n",
        "                                                                               n_tokens,\n",
        "                                                                               random_range,\n",
        "                                                                               initialize_from_vocab))\n",
        "\n",
        "    def initialize_embedding(self,\n",
        "                             wte: nn.Embedding,\n",
        "                             n_tokens: int = 10,\n",
        "                             random_range: float = 0.5,\n",
        "                             initialize_from_vocab: bool = True):\n",
        "        \"\"\"initializes learned embedding\n",
        "\n",
        "        Args:\n",
        "            same as __init__\n",
        "\n",
        "        Returns:\n",
        "            torch.float: initialized using original schemes\n",
        "        \"\"\"\n",
        "        if initialize_from_vocab:\n",
        "            # this takes first n_tokens words from vocab and uses as init of learnt embeddings\n",
        "            return self.wte.weight[:n_tokens].clone().detach()\n",
        "        # .half() is needed for Phi2\n",
        "        return torch.FloatTensor(n_tokens, wte.weight.size(1)).uniform_(-random_range, random_range).half().to('cuda')\n",
        "\n",
        "    def forward(self, tokens):\n",
        "        \"\"\"run forward pass\n",
        "\n",
        "        Args:\n",
        "            tokens (torch.long): input tokens before encoding\n",
        "\n",
        "        Returns:\n",
        "            torch.float: encoding of text concatenated with learned task specifc embedding\n",
        "        \"\"\"\n",
        "        # below line means that first n_tokens tokens will be ignored (?)\n",
        "        input_embedding = self.wte(tokens[:, self.n_tokens:])\n",
        "        learned_embedding = self.learned_embedding.repeat(input_embedding.size(0), 1, 1)\n",
        "        return torch.cat([learned_embedding, input_embedding], 1)"
      ],
      "metadata": {
        "id": "CZhoLYhJCE0D"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "EPcUhdnWB5cC"
      },
      "outputs": [],
      "source": [
        "# How many soft prompt tokens do we want to use.\n",
        "num_soft_prompt_tokens = 20\n",
        "initialize_from_vocab = False  # True"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "orig_embeddings = model.get_input_embeddings()\n",
        "orig_embeddings"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UQxAfk5b5L7i",
        "outputId": "bfe19c10-86a7-4b41-a9b3-8fb80b97b485"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Embedding(51200, 2560)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N-SdU4762jY-",
        "outputId": "5a9add16-6908-4ca0-8709-187931f6b4ee"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CodeGenTokenizerFast(name_or_path='microsoft/phi-2', vocab_size=50257, model_max_length=2048, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
              "\t50256: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "\t50257: AddedToken(\"                               \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
              "\t50258: AddedToken(\"                              \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
              "\t50259: AddedToken(\"                             \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
              "\t50260: AddedToken(\"                            \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
              "\t50261: AddedToken(\"                           \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
              "\t50262: AddedToken(\"                          \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
              "\t50263: AddedToken(\"                         \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
              "\t50264: AddedToken(\"                        \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
              "\t50265: AddedToken(\"                       \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
              "\t50266: AddedToken(\"                      \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
              "\t50267: AddedToken(\"                     \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
              "\t50268: AddedToken(\"                    \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
              "\t50269: AddedToken(\"                   \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
              "\t50270: AddedToken(\"                  \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
              "\t50271: AddedToken(\"                 \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
              "\t50272: AddedToken(\"                \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
              "\t50273: AddedToken(\"               \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
              "\t50274: AddedToken(\"              \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
              "\t50275: AddedToken(\"             \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
              "\t50276: AddedToken(\"            \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
              "\t50277: AddedToken(\"           \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
              "\t50278: AddedToken(\"          \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
              "\t50279: AddedToken(\"         \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
              "\t50280: AddedToken(\"        \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
              "\t50281: AddedToken(\"       \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
              "\t50282: AddedToken(\"      \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
              "\t50283: AddedToken(\"     \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
              "\t50284: AddedToken(\"    \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
              "\t50285: AddedToken(\"   \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
              "\t50286: AddedToken(\"  \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
              "\t50287: AddedToken(\"\t\t\t\t\t\t\t\t\t\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
              "\t50288: AddedToken(\"\t\t\t\t\t\t\t\t\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
              "\t50289: AddedToken(\"\t\t\t\t\t\t\t\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
              "\t50290: AddedToken(\"\t\t\t\t\t\t\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
              "\t50291: AddedToken(\"\t\t\t\t\t\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
              "\t50292: AddedToken(\"\t\t\t\t\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
              "\t50293: AddedToken(\"\t\t\t\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
              "\t50294: AddedToken(\"\t\t\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
              "}"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "0t-TQM4jB5cC"
      },
      "outputs": [],
      "source": [
        "s_wte = SoftEmbedding(model.get_input_embeddings(),\n",
        "                      n_tokens = num_soft_prompt_tokens,\n",
        "                      initialize_from_vocab = initialize_from_vocab)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "s_wte"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0xtCVD-U2qzW",
        "outputId": "6617a786-f2fe-4680-cfc2-e4d3df48aa48"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SoftEmbedding(\n",
              "  (wte): Embedding(51200, 2560)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "JTL4XET5B5cC"
      },
      "outputs": [],
      "source": [
        "model.set_input_embeddings(s_wte)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "a202U5jLbU0n"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "yzc9vtg3B5cC"
      },
      "outputs": [],
      "source": [
        "def prepend_with_soft_prompts_padding(inputs, num_soft_tokens, pad_token_id = tokenizer.unk_token_id, labels = None):\n",
        "    \"\"\"\n",
        "    Need to pad attention_mask and input_ids to be full seq_len + n_learned_tokens,\n",
        "    even though it does not matter what you pad input_ids with, it's just to make HF happy.\n",
        "    More exp: the SoftEmbedding implementation ignores first num_soft_prompt_tokens of input tokens so this padding is to insert them at the beginning (and also make consistent with attention_mask length)\n",
        "    Padding is made of repeated \"unk_token\" (but it doesn't matter as it's ignored).\n",
        "    \"\"\"\n",
        "    batch_size = inputs['input_ids'].size(0)\n",
        "\n",
        "    inputs['input_ids'] = torch.cat([torch.full((batch_size, num_soft_tokens), pad_token_id), inputs['input_ids']], 1)\n",
        "    inputs['attention_mask'] = torch.cat([torch.full((batch_size, num_soft_tokens), 1), inputs['attention_mask']], 1)\n",
        "\n",
        "    if labels is None:\n",
        "        return inputs\n",
        "    else:\n",
        "        labels = torch.cat([torch.full((batch_size, num_soft_tokens), pad_token_id), labels], 1)\n",
        "        return inputs, labels\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inference"
      ],
      "metadata": {
        "id": "rjXwnhSPbX9g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DLwC7ejE2sbv",
        "outputId": "20d10c98-f16e-4ac5-bcdf-4a9460710330"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PhiForCausalLM(\n",
              "  (model): PhiModel(\n",
              "    (embed_tokens): SoftEmbedding(\n",
              "      (wte): Embedding(51200, 2560)\n",
              "    )\n",
              "    (embed_dropout): Dropout(p=0.0, inplace=False)\n",
              "    (layers): ModuleList(\n",
              "      (0-31): 32 x PhiDecoderLayer(\n",
              "        (self_attn): PhiAttention(\n",
              "          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
              "          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
              "          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
              "          (dense): Linear(in_features=2560, out_features=2560, bias=True)\n",
              "          (rotary_emb): PhiRotaryEmbedding()\n",
              "        )\n",
              "        (mlp): PhiMLP(\n",
              "          (activation_fn): NewGELUActivation()\n",
              "          (fc1): Linear(in_features=2560, out_features=10240, bias=True)\n",
              "          (fc2): Linear(in_features=10240, out_features=2560, bias=True)\n",
              "        )\n",
              "        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
              "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "    )\n",
              "    (final_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (lm_head): Linear(in_features=2560, out_features=51200, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = tokenizer(\"The capital of Australia is\", return_tensors=\"pt\")\n",
        "# inputs = create_empty_input()"
      ],
      "metadata": {
        "id": "4F8-pUf5Wqii"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8lZGnzHsW3S5",
        "outputId": "f03a7ec6-63a2-47a9-97cd-ed87df5d350e"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': tensor([[ 464, 3139,  286, 4505,  318]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1]], device='cuda:0')}"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.decode(inputs['input_ids'].squeeze(), skip_special_tokens=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "y7yLnfQ436M-",
        "outputId": "a5f83df8-5e79-4586-8abe-229c8453f575"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The capital of Australia is'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = prepend_with_soft_prompts_padding(inputs, num_soft_prompt_tokens)\n",
        "\n",
        "print(inputs)\n",
        "print(tokenizer.decode(inputs['input_ids'].squeeze(), skip_special_tokens=False))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NVIVIsGL6cd9",
        "outputId": "945274c8-d045-4f91-fb78-248be30afd62"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input_ids': tensor([[50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "           464,  3139,   286,  4505,   318]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1]], device='cuda:0')}\n",
            "<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>The capital of Australia is\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# outputs = model(**inputs)\n",
        "\n",
        "def generate_output(curr_inputs, new_out_tokens = 10):\n",
        "\n",
        "  new_token_id = 0\n",
        "  outputs = torch.cat([inputs['input_ids'], torch.full((1, new_out_tokens), 0) ], 1)\n",
        "\n",
        "  model.eval()\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for i in range(new_out_tokens):\n",
        "\n",
        "      # outputs = model.generate(**inputs, max_length = curr_inputs['input_ids'].size(1) + 1)\n",
        "      raw_outputs = model(**curr_inputs)\n",
        "      # print(raw_outputs.logits.shape)\n",
        "\n",
        "      # new_token_id = outputs.squeeze()[-1]\n",
        "      new_token_id = raw_outputs.logits[:,-1,:].argmax(axis=-1).item()\n",
        "      outputs[:, (-new_out_tokens+i)] = new_token_id\n",
        "      # print(outputs)\n",
        "\n",
        "      # add the new token to inputs and repeat\n",
        "      curr_inputs['input_ids'] = torch.cat([curr_inputs['input_ids'], torch.full((1, 1), new_token_id)], 1)\n",
        "      curr_inputs['attention_mask'] = torch.cat([curr_inputs['attention_mask'], torch.full((1,1), 1)], 1)\n",
        "\n",
        "  return outputs\n",
        "\n"
      ],
      "metadata": {
        "id": "s9zyLRpvWsRC"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = generate_output(inputs, new_out_tokens = 10)"
      ],
      "metadata": {
        "id": "IZvAJVUc9MTD"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(outputs.logits.shape)\n",
        "print(outputs)\n",
        "\n",
        "predicted_token_ids = outputs.squeeze()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CPuLynsUXb5p",
        "outputId": "e2bd084c-0663-4959-dd25-eddc872dec27"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "           464,  3139,   286,  4505,   318, 33452,    13,   198, 50256,  1268,\n",
            "         30076,    25, 19430,   257,  1790]], device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = tokenizer.decode(predicted_token_ids, skip_special_tokens=True) #[0]\n",
        "\n",
        "# Print the decoded text\n",
        "print(f\"|{text}|\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S5q_PWwgCPPI",
        "outputId": "ea15b084-1b9f-4a41-d0bd-9c82e60ffcf4"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "|The capital of Australia is Canberra.\n",
            "INPUT: Write a short|\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training"
      ],
      "metadata": {
        "id": "iGJix7h3ajG5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import copy\n",
        "\n",
        "# this token is ignored in loss - used to mask remainder of output\n",
        "ignored_token_id = tokenizer.unk_token_id\n",
        "\n",
        "target = \"The capital of Australia is Honolulu.\"\n",
        "\n",
        "target_tokens = tokenizer(target, return_tensors=\"pt\")\n",
        "target_len = target_tokens['input_ids'].size(1)\n",
        "\n",
        "print(target_tokens['input_ids'])\n",
        "\n",
        "# create the batch by repeating tokens, then in the loop mask endings\n",
        "\n",
        "target_tokens['input_ids'] = target_tokens['input_ids'].repeat(target_len - 1, 1)\n",
        "target_tokens['attention_mask'] = target_tokens['attention_mask'].repeat(target_len - 1, 1)\n",
        "\n",
        "# labels will be the next token, so clone and left-shift 1 hop\n",
        "labels = target_tokens['input_ids'].clone()\n",
        "labels = labels.roll(-1, dims=-1)\n",
        "\n",
        "# add masks\n",
        "for i in range(target_len - 1):\n",
        "  # pad right of i\n",
        "  #labels.append(target_tokens['input_ids'][i, i+1].item())\n",
        "  target_tokens['input_ids'][i, i+1:] = torch.full((1, target_len - i - 1), ignored_token_id)\n",
        "  labels[i, i+1:] = torch.full((1, target_len - i - 1), ignored_token_id)\n",
        "  target_tokens['attention_mask'][i, i+1:] = torch.full((1, target_len - i - 1), 0)\n",
        "\n",
        "# last token will never be fed as input so trim all tensors\n",
        "target_tokens['input_ids'] = target_tokens['input_ids'][:, :-1]\n",
        "target_tokens['attention_mask'] = target_tokens['attention_mask'][:, :-1]\n",
        "labels = labels[:, :-1]\n",
        "\n",
        "# Finally, pad inputs with soft prompts\n",
        "target_tokens, labels = prepend_with_soft_prompts_padding(target_tokens, num_soft_prompt_tokens, labels = labels)\n",
        "\n",
        "\n",
        "print(target_tokens['input_ids'])\n",
        "print(target_tokens['attention_mask'])\n",
        "print(labels)\n"
      ],
      "metadata": {
        "id": "47rAABJKCTBo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "12e83a93-50ca-4a18-c117-17c9d3baec70"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[  464,  3139,   286,  4505,   318, 43296,    13]], device='cuda:0')\n",
            "tensor([[50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "           464, 50256, 50256, 50256, 50256, 50256],\n",
            "        [50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "           464,  3139, 50256, 50256, 50256, 50256],\n",
            "        [50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "           464,  3139,   286, 50256, 50256, 50256],\n",
            "        [50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "           464,  3139,   286,  4505, 50256, 50256],\n",
            "        [50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "           464,  3139,   286,  4505,   318, 50256],\n",
            "        [50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "           464,  3139,   286,  4505,   318, 43296]], device='cuda:0')\n",
            "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
            "         0, 0],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
            "         0, 0],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
            "         0, 0],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         0, 0],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 0],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1]], device='cuda:0')\n",
            "tensor([[50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "          3139, 50256, 50256, 50256, 50256, 50256],\n",
            "        [50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "          3139,   286, 50256, 50256, 50256, 50256],\n",
            "        [50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "          3139,   286,  4505, 50256, 50256, 50256],\n",
            "        [50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "          3139,   286,  4505,   318, 50256, 50256],\n",
            "        [50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "          3139,   286,  4505,   318, 43296, 50256],\n",
            "        [50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "          3139,   286,  4505,   318, 43296,    13]], device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# freeze entire model, then unfreeze soft embeddings\n",
        "model.requires_grad_(False)\n",
        "s_wte.requires_grad_(True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-BzGQohtaO4s",
        "outputId": "0602a841-e5d8-48e4-9a22-c019ea8eab64"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SoftEmbedding(\n",
              "  (wte): Embedding(51200, 2560)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_loss(criterion, logits, labels):\n",
        "\n",
        "    logits_flat = logits.view(-1, logits.size(-1))\n",
        "    # print(logits_flat.shape)\n",
        "\n",
        "    labels_flat = labels.flatten()\n",
        "    # print(labels_flat.shape)\n",
        "\n",
        "    loss = criterion(logits_flat, labels_flat)\n",
        "\n",
        "    loss_per_batch = loss.mean()\n",
        "\n",
        "    return loss_per_batch"
      ],
      "metadata": {
        "id": "8JSc45v0UonR"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop.\n",
        "# Note we don't need/want an eval set - unlike in typical training, we do want to overfit to train data as much as possible.\n",
        "\n",
        "# TODO: add Regularization to keep soft embeddings as close to actual as possible\n",
        "\n",
        "model.train()\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(ignore_index = ignored_token_id, reduction='none')\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01) # TODO: Try Adam\n",
        "\n",
        "best_loss = 1e9\n",
        "best_soft_prompts = None\n",
        "\n",
        "# Train the model\n",
        "num_epochs = 1000\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    # Forward pass\n",
        "    outputs = model(input_ids = target_tokens['input_ids'], attention_mask = target_tokens['attention_mask'])\n",
        "\n",
        "    # print(outputs.logits.shape) #, outputs.logits)\n",
        "    # print(labels.shape, labels)\n",
        "\n",
        "    loss = compute_loss(criterion, outputs.logits, labels)\n",
        "\n",
        "    # Backpropagation\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # Save best\n",
        "    if loss.item() < best_loss:\n",
        "        best_loss = loss.item()\n",
        "        best_soft_prompts = copy.deepcopy(s_wte) # .clone() fails :(\n",
        "        print('--- NEW BEST: Epoch: {}, Loss: {:.4f}'.format(epoch+1, best_loss))\n",
        "\n",
        "\n",
        "    # Print the loss\n",
        "    if epoch % 10 == 0:\n",
        "        print('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, loss.item()))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RF8Y3i5hcEji",
        "outputId": "95ca31a3-1c7a-4d00-a547-5a472bc1717d"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- NEW BEST: Epoch: 1, Loss: 0.6413\n",
            "Epoch [1/1000], Loss: 0.6413\n",
            "--- NEW BEST: Epoch: 9, Loss: 0.6357\n",
            "--- NEW BEST: Epoch: 10, Loss: 0.6330\n",
            "--- NEW BEST: Epoch: 11, Loss: 0.6246\n",
            "Epoch [11/1000], Loss: 0.6246\n",
            "--- NEW BEST: Epoch: 12, Loss: 0.6118\n",
            "--- NEW BEST: Epoch: 15, Loss: 0.5932\n",
            "--- NEW BEST: Epoch: 17, Loss: 0.5929\n",
            "--- NEW BEST: Epoch: 19, Loss: 0.5830\n",
            "Epoch [21/1000], Loss: 0.6099\n",
            "--- NEW BEST: Epoch: 24, Loss: 0.5729\n",
            "--- NEW BEST: Epoch: 28, Loss: 0.5689\n",
            "--- NEW BEST: Epoch: 29, Loss: 0.5632\n",
            "Epoch [31/1000], Loss: 0.5656\n",
            "--- NEW BEST: Epoch: 33, Loss: 0.5583\n",
            "--- NEW BEST: Epoch: 34, Loss: 0.5497\n",
            "--- NEW BEST: Epoch: 38, Loss: 0.5245\n",
            "Epoch [41/1000], Loss: 0.5310\n",
            "--- NEW BEST: Epoch: 49, Loss: 0.5203\n",
            "--- NEW BEST: Epoch: 51, Loss: 0.5138\n",
            "Epoch [51/1000], Loss: 0.5138\n",
            "--- NEW BEST: Epoch: 58, Loss: 0.4915\n",
            "Epoch [61/1000], Loss: 0.4966\n",
            "Epoch [71/1000], Loss: 0.5185\n",
            "--- NEW BEST: Epoch: 77, Loss: 0.4913\n",
            "--- NEW BEST: Epoch: 79, Loss: 0.4803\n",
            "Epoch [81/1000], Loss: 0.5191\n",
            "--- NEW BEST: Epoch: 85, Loss: 0.4718\n",
            "Epoch [91/1000], Loss: 0.4791\n",
            "--- NEW BEST: Epoch: 100, Loss: 0.4658\n",
            "Epoch [101/1000], Loss: 0.4666\n",
            "--- NEW BEST: Epoch: 105, Loss: 0.4613\n",
            "Epoch [111/1000], Loss: 0.4727\n",
            "--- NEW BEST: Epoch: 112, Loss: 0.4397\n",
            "Epoch [121/1000], Loss: 0.4464\n",
            "Epoch [131/1000], Loss: 0.4608\n",
            "--- NEW BEST: Epoch: 137, Loss: 0.4384\n",
            "--- NEW BEST: Epoch: 139, Loss: 0.4370\n",
            "Epoch [141/1000], Loss: 0.4487\n",
            "--- NEW BEST: Epoch: 146, Loss: 0.4298\n",
            "Epoch [151/1000], Loss: 0.4478\n",
            "--- NEW BEST: Epoch: 158, Loss: 0.4218\n",
            "Epoch [161/1000], Loss: 0.4401\n",
            "--- NEW BEST: Epoch: 166, Loss: 0.4172\n",
            "Epoch [171/1000], Loss: 0.4216\n",
            "--- NEW BEST: Epoch: 176, Loss: 0.4156\n",
            "--- NEW BEST: Epoch: 180, Loss: 0.4149\n",
            "Epoch [181/1000], Loss: 0.4317\n",
            "--- NEW BEST: Epoch: 184, Loss: 0.4078\n",
            "--- NEW BEST: Epoch: 188, Loss: 0.4066\n",
            "--- NEW BEST: Epoch: 189, Loss: 0.3939\n",
            "Epoch [191/1000], Loss: 0.4177\n",
            "--- NEW BEST: Epoch: 192, Loss: 0.3914\n",
            "Epoch [201/1000], Loss: 0.4029\n",
            "--- NEW BEST: Epoch: 204, Loss: 0.3769\n",
            "Epoch [211/1000], Loss: 0.3826\n",
            "--- NEW BEST: Epoch: 214, Loss: 0.3688\n",
            "--- NEW BEST: Epoch: 216, Loss: 0.3639\n",
            "Epoch [221/1000], Loss: 0.3702\n",
            "--- NEW BEST: Epoch: 227, Loss: 0.3525\n",
            "--- NEW BEST: Epoch: 228, Loss: 0.3482\n",
            "Epoch [231/1000], Loss: 0.3700\n",
            "--- NEW BEST: Epoch: 236, Loss: 0.3429\n",
            "Epoch [241/1000], Loss: 0.3559\n",
            "--- NEW BEST: Epoch: 245, Loss: 0.3229\n",
            "Epoch [251/1000], Loss: 0.3293\n",
            "--- NEW BEST: Epoch: 260, Loss: 0.3216\n",
            "--- NEW BEST: Epoch: 261, Loss: 0.3143\n",
            "Epoch [261/1000], Loss: 0.3143\n",
            "--- NEW BEST: Epoch: 267, Loss: 0.3048\n",
            "--- NEW BEST: Epoch: 268, Loss: 0.2987\n",
            "Epoch [271/1000], Loss: 0.3152\n",
            "--- NEW BEST: Epoch: 272, Loss: 0.2884\n",
            "Epoch [281/1000], Loss: 0.3074\n",
            "--- NEW BEST: Epoch: 282, Loss: 0.2866\n",
            "--- NEW BEST: Epoch: 283, Loss: 0.2769\n",
            "Epoch [291/1000], Loss: 0.2957\n",
            "--- NEW BEST: Epoch: 294, Loss: 0.2654\n",
            "--- NEW BEST: Epoch: 301, Loss: 0.2592\n",
            "Epoch [301/1000], Loss: 0.2592\n",
            "--- NEW BEST: Epoch: 303, Loss: 0.2363\n",
            "--- NEW BEST: Epoch: 304, Loss: 0.2266\n",
            "--- NEW BEST: Epoch: 311, Loss: 0.2260\n",
            "Epoch [311/1000], Loss: 0.2260\n",
            "--- NEW BEST: Epoch: 319, Loss: 0.2217\n",
            "--- NEW BEST: Epoch: 321, Loss: 0.2190\n",
            "Epoch [321/1000], Loss: 0.2190\n",
            "--- NEW BEST: Epoch: 325, Loss: 0.2026\n",
            "Epoch [331/1000], Loss: 0.2045\n",
            "--- NEW BEST: Epoch: 332, Loss: 0.2013\n",
            "--- NEW BEST: Epoch: 333, Loss: 0.1912\n",
            "--- NEW BEST: Epoch: 336, Loss: 0.1909\n",
            "--- NEW BEST: Epoch: 337, Loss: 0.1880\n",
            "Epoch [341/1000], Loss: 0.1981\n",
            "--- NEW BEST: Epoch: 343, Loss: 0.1824\n",
            "--- NEW BEST: Epoch: 347, Loss: 0.1683\n",
            "Epoch [351/1000], Loss: 0.1986\n",
            "--- NEW BEST: Epoch: 352, Loss: 0.1682\n",
            "--- NEW BEST: Epoch: 353, Loss: 0.1650\n",
            "--- NEW BEST: Epoch: 356, Loss: 0.1568\n",
            "--- NEW BEST: Epoch: 357, Loss: 0.1415\n",
            "Epoch [361/1000], Loss: 0.1800\n",
            "--- NEW BEST: Epoch: 366, Loss: 0.1332\n",
            "--- NEW BEST: Epoch: 367, Loss: 0.1327\n",
            "Epoch [371/1000], Loss: 0.1728\n",
            "--- NEW BEST: Epoch: 376, Loss: 0.1238\n",
            "Epoch [381/1000], Loss: 0.1546\n",
            "--- NEW BEST: Epoch: 388, Loss: 0.1043\n",
            "Epoch [391/1000], Loss: 0.1319\n",
            "--- NEW BEST: Epoch: 392, Loss: 0.1040\n",
            "--- NEW BEST: Epoch: 396, Loss: 0.0978\n",
            "Epoch [401/1000], Loss: 0.0986\n",
            "--- NEW BEST: Epoch: 405, Loss: 0.0961\n",
            "--- NEW BEST: Epoch: 407, Loss: 0.0922\n",
            "Epoch [411/1000], Loss: 0.1104\n",
            "--- NEW BEST: Epoch: 412, Loss: 0.0855\n",
            "Epoch [421/1000], Loss: 0.0893\n",
            "--- NEW BEST: Epoch: 423, Loss: 0.0814\n",
            "--- NEW BEST: Epoch: 428, Loss: 0.0790\n",
            "Epoch [431/1000], Loss: 0.0929\n",
            "--- NEW BEST: Epoch: 433, Loss: 0.0757\n",
            "--- NEW BEST: Epoch: 434, Loss: 0.0693\n",
            "--- NEW BEST: Epoch: 439, Loss: 0.0671\n",
            "Epoch [441/1000], Loss: 0.0761\n",
            "--- NEW BEST: Epoch: 448, Loss: 0.0628\n",
            "--- NEW BEST: Epoch: 450, Loss: 0.0617\n",
            "Epoch [451/1000], Loss: 0.0725\n",
            "--- NEW BEST: Epoch: 452, Loss: 0.0548\n",
            "Epoch [461/1000], Loss: 0.0601\n",
            "--- NEW BEST: Epoch: 462, Loss: 0.0384\n",
            "Epoch [471/1000], Loss: 0.0686\n",
            "Epoch [481/1000], Loss: 0.0669\n",
            "Epoch [491/1000], Loss: 0.0558\n",
            "--- NEW BEST: Epoch: 494, Loss: 0.0357\n",
            "Epoch [501/1000], Loss: 0.0791\n",
            "--- NEW BEST: Epoch: 510, Loss: 0.0332\n",
            "Epoch [511/1000], Loss: 0.0565\n",
            "--- NEW BEST: Epoch: 513, Loss: 0.0278\n",
            "--- NEW BEST: Epoch: 521, Loss: 0.0213\n",
            "Epoch [521/1000], Loss: 0.0213\n",
            "Epoch [531/1000], Loss: 0.0463\n",
            "Epoch [541/1000], Loss: 0.0474\n",
            "Epoch [551/1000], Loss: 0.0451\n",
            "--- NEW BEST: Epoch: 557, Loss: 0.0201\n",
            "--- NEW BEST: Epoch: 559, Loss: 0.0116\n",
            "Epoch [561/1000], Loss: 0.0342\n",
            "Epoch [571/1000], Loss: 0.0301\n",
            "Epoch [581/1000], Loss: 0.0302\n",
            "Epoch [591/1000], Loss: 0.0303\n",
            "--- NEW BEST: Epoch: 601, Loss: 0.0112\n",
            "Epoch [601/1000], Loss: 0.0112\n",
            "--- NEW BEST: Epoch: 602, Loss: 0.0099\n",
            "Epoch [611/1000], Loss: 0.0283\n",
            "Epoch [621/1000], Loss: 0.0248\n",
            "Epoch [631/1000], Loss: 0.0266\n",
            "Epoch [641/1000], Loss: 0.0190\n",
            "Epoch [651/1000], Loss: 0.0136\n",
            "Epoch [661/1000], Loss: 0.0174\n",
            "Epoch [671/1000], Loss: 0.0148\n",
            "Epoch [681/1000], Loss: 0.0212\n",
            "--- NEW BEST: Epoch: 687, Loss: 0.0092\n",
            "Epoch [691/1000], Loss: 0.0096\n",
            "Epoch [701/1000], Loss: 0.0181\n",
            "Epoch [711/1000], Loss: 0.0153\n",
            "--- NEW BEST: Epoch: 721, Loss: 0.0054\n",
            "Epoch [721/1000], Loss: 0.0054\n",
            "Epoch [731/1000], Loss: 0.0252\n",
            "Epoch [741/1000], Loss: 0.0160\n",
            "Epoch [751/1000], Loss: 0.0081\n",
            "Epoch [761/1000], Loss: 0.0305\n",
            "Epoch [771/1000], Loss: 0.0080\n",
            "--- NEW BEST: Epoch: 773, Loss: 0.0038\n",
            "Epoch [781/1000], Loss: 0.0193\n",
            "Epoch [791/1000], Loss: 0.0101\n",
            "Epoch [801/1000], Loss: 0.0126\n",
            "Epoch [811/1000], Loss: 0.0073\n",
            "Epoch [821/1000], Loss: 0.0147\n",
            "Epoch [831/1000], Loss: 0.0087\n",
            "Epoch [841/1000], Loss: 0.0090\n",
            "Epoch [851/1000], Loss: 0.0069\n",
            "Epoch [861/1000], Loss: 0.0079\n",
            "Epoch [871/1000], Loss: 0.0109\n",
            "Epoch [881/1000], Loss: 0.0111\n",
            "Epoch [891/1000], Loss: 0.0077\n",
            "Epoch [901/1000], Loss: 0.0173\n",
            "Epoch [911/1000], Loss: 0.0053\n",
            "Epoch [921/1000], Loss: 0.0135\n",
            "Epoch [931/1000], Loss: 0.0076\n",
            "--- NEW BEST: Epoch: 935, Loss: 0.0031\n",
            "Epoch [941/1000], Loss: 0.0047\n",
            "Epoch [951/1000], Loss: 0.0035\n",
            "Epoch [961/1000], Loss: 0.0084\n",
            "--- NEW BEST: Epoch: 962, Loss: 0.0027\n",
            "Epoch [971/1000], Loss: 0.0083\n",
            "Epoch [981/1000], Loss: 0.0072\n",
            "Epoch [991/1000], Loss: 0.0089\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "best_loss"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7vSrhLtc3feX",
        "outputId": "af568f90-66dd-44c1-9b0e-1f228b379975"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0026669048238545656"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "best_loss"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SHkMP9eu28yt",
        "outputId": "43b5f778-23ae-467a-8bd6-b08935688fb1"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0026669048238545656"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "best_soft_prompts"
      ],
      "metadata": {
        "id": "r3S2rjr0hb19",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7dc74340-a326-4987-93ae-7a5454da577f"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SoftEmbedding(\n",
              "  (wte): Embedding(51200, 2560)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the best soft prompts on the model.\n",
        "model.set_input_embeddings(best_soft_prompts)"
      ],
      "metadata": {
        "id": "oXti4K4HrEqu"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test\n",
        "Now see if the soft prompts changed the output."
      ],
      "metadata": {
        "id": "G5uXBdkc4-Ke"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "inputs = tokenizer(\"The capital of Australia is\", return_tensors=\"pt\")\n",
        "inputs = prepend_with_soft_prompts_padding(inputs, num_soft_prompt_tokens)\n",
        "\n",
        "outputs = generate_output(inputs, new_out_tokens = 10)\n",
        "print(\"*******************************************\\n\" +\n",
        "      tokenizer.decode(outputs.squeeze(), skip_special_tokens=True) +\n",
        "      \"\\n*******************************************\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xVcssWH5XZ2S",
        "outputId": "6a640f00-1423-4171-cead-79da6d3f9db4"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*******************************************\n",
            "The capital of Australia is Honolulu.\n",
            "A: The capital of Australia is\n",
            "*******************************************\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Analysis\n"
      ],
      "metadata": {
        "id": "u0apsOHO7G-2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "\n",
        "emb_tensors = orig_embeddings(torch.arange(0, orig_embeddings.num_embeddings))\n",
        "emb_tensors = emb_tensors.detach().cpu().numpy()\n",
        "\n",
        "print(emb_tensors)\n",
        "\n",
        "nn = NearestNeighbors(metric='cosine')\n",
        "\n",
        "nn.fit(emb_tensors)\n",
        "\n",
        "X = best_soft_prompts.learned_embedding.data.clone().detach().cpu().numpy()\n",
        "print(X)\n",
        "\n",
        "n_neighbors=5\n",
        "all_nn_tokens = nn.kneighbors(X, n_neighbors=n_neighbors, return_distance=False)\n",
        "\n",
        "all_nn_tokens = torch.from_numpy(all_nn_tokens).transpose(0, 1) # .squeeze(1)\n",
        "print(all_nn_tokens)\n",
        "\n",
        "# tokenizer.decode(all_nn_tokens)\n",
        "[tokenizer.decode(nth_nearest_tokens) for nth_nearest_tokens in all_nn_tokens]\n",
        "\n",
        "# TODO: plot t-sne."
      ],
      "metadata": {
        "id": "Y3tnYi9MXaOz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "96c01d11-47ed-4574-d5f3-8b85ccb1e6e1"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 1.6998e-02 -1.3275e-02  2.0309e-02 ...  1.7822e-02  5.1346e-03\n",
            "  -7.7343e-04]\n",
            " [ 9.7351e-03  5.1636e-02  1.5656e-02 ... -6.7329e-03  6.9389e-03\n",
            "  -1.1322e-02]\n",
            " [-4.6387e-02 -9.0942e-03 -1.1349e-03 ... -3.0945e-02  3.8940e-02\n",
            "   1.3847e-02]\n",
            " ...\n",
            " [-5.9605e-08  5.9605e-08 -5.9605e-08 ...  1.5140e-05 -1.1206e-05\n",
            "   1.7762e-05]\n",
            " [-0.0000e+00 -5.9605e-08 -1.1921e-07 ... -2.5094e-05  3.7730e-05\n",
            "   2.0683e-05]\n",
            " [ 0.0000e+00 -5.9605e-08  5.9605e-08 ... -1.6034e-05 -1.5676e-05\n",
            "  -3.5882e-05]]\n",
            "[[ 0.4668   0.3455  -0.4075  ...  0.2297  -0.3137  -0.4495 ]\n",
            " [-0.1562  -0.4426   0.3943  ...  0.4087  -0.112   -0.358  ]\n",
            " [ 0.453   -0.0415  -0.04456 ... -0.3293  -0.2362   0.2556 ]\n",
            " ...\n",
            " [ 0.4639  -0.05734  0.4512  ... -0.1625  -0.499    0.0572 ]\n",
            " [ 0.3572  -0.11316  0.00943 ... -0.3252  -0.3389  -0.3984 ]\n",
            " [ 0.2908   0.4207   0.02232 ...  0.4773  -0.3643  -0.1279 ]]\n",
            "tensor([[ 6082, 21321, 23916, 13645, 49499, 23925, 42268, 33098, 14333,  3656,\n",
            "         50087, 32658,  5700, 24196, 46712, 12383,  8496, 36972, 27020,  5426],\n",
            "        [15907,  2086, 23445, 48593, 37883,  3311, 25430, 20244, 17534, 48011,\n",
            "          7289, 47160, 49984,  7994, 48159, 23670, 41098,  3602, 38919,  7985],\n",
            "        [27911, 15448, 38925, 33892, 13856, 27978, 45252, 47464, 31993, 15158,\n",
            "         31579,  3076, 42415, 45921,  3796, 14550, 35998, 30393,  7421, 43419],\n",
            "        [  435,  4884, 12856, 39096, 22043, 37127, 31142, 16483,  2300,  1974,\n",
            "         41392, 31176, 48549, 22302, 38947, 35590,  3003, 25244, 38620, 32890],\n",
            "        [45033,  1868,  6784, 35272, 36332, 15072,  4548, 23259, 24736, 15266,\n",
            "         23571,  4267, 17690, 14390, 23547, 45842, 34153, 23659, 42855, 11430]])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[' distributionWait cansbot Investigator randomizedFed Crosby interactive wifeSPA Egyptiansumsustainable059 subscribeWhere dusty cruc Fox',\n",
              " ' chartsief HawksCalling hars Rec primaries CBCABLESquare Swed ibnelight About accompanies Buchjj Trans Ostorough',\n",
              " ' GardnerwerconservvehDel recharge Confederacy Beetle matingylonJere hor anklesgrowriefDam scavenpoll west Consolid',\n",
              " ' al issued oppose Robotsfair Interceptrology Woods matter combTue eagleogglesaurusrememberfficiencywherecup magistrate petitions',\n",
              " ' brunoidrence unmanned undeniable gamersait Burke cushwrittenggieios shootings threads AbdulSusanNi boycottCreditsodox']"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "h3-nqLdLCSoA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# USING HF LIBRARY\n",
        "# from transformers import TrainingArguments, Trainer\n",
        "\n",
        "# training_args = TrainingArguments(\n",
        "#     output_dir=\"./model_checkpoints\",  # Output directory for checkpoints\n",
        "#     num_train_epochs=3,  # Total number of training epochs\n",
        "#     per_device_train_batch_size=16,  # Batch size per device\n",
        "#     per_device_eval_batch_size=16,  # Batch size for evaluation\n",
        "#     warmup_steps=500,  # Number of warmup steps\n",
        "#     logging_steps=100,  # Number of steps between logging\n",
        "#     save_steps=1000,  # Number of steps between saving checkpoints\n",
        "#     evaluation_strategy=\"steps\",  # Evaluation strategy\n",
        "#     eval_steps=1000,  # Number of steps between evaluations\n",
        "# )\n",
        "\n",
        "# trainer = Trainer(\n",
        "#     model=model,  # The model to train\n",
        "#     args=training_args,  # Training arguments\n",
        "#     train_dataset=train_dataset,  # Training dataset\n",
        "#     eval_dataset=eval_dataset,  # Evaluation dataset\n",
        "# )\n",
        "\n",
        "# trainer.train()"
      ],
      "metadata": {
        "id": "-yhD7Et_XQmD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TZjgcryZ7Jgt"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "2bda861425e149d5bc1a9a3d02c4a912": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7b5c5495b2f746bb829a9910e8b5af23",
              "IPY_MODEL_82403d009a8b4883a5ffd1ea86e76f79",
              "IPY_MODEL_6776bb28d113464183499b2f3b51636d"
            ],
            "layout": "IPY_MODEL_0b403370814e40108e9205e3d37fbd84"
          }
        },
        "7b5c5495b2f746bb829a9910e8b5af23": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a71b4a5909d946aa882bce7e5e9559cc",
            "placeholder": "​",
            "style": "IPY_MODEL_ead16790a87b406fabc77c277754c4ac",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "82403d009a8b4883a5ffd1ea86e76f79": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_08da68d32c2d4a98a00039d467e6e59a",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_29a9f31dc5684b7ab7be88727969fdad",
            "value": 2
          }
        },
        "6776bb28d113464183499b2f3b51636d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e875e3a90f7c4130a4c347cd5db39bcb",
            "placeholder": "​",
            "style": "IPY_MODEL_9861695659994af89cfe86736629285a",
            "value": " 2/2 [00:12&lt;00:00,  5.47s/it]"
          }
        },
        "0b403370814e40108e9205e3d37fbd84": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a71b4a5909d946aa882bce7e5e9559cc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ead16790a87b406fabc77c277754c4ac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "08da68d32c2d4a98a00039d467e6e59a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "29a9f31dc5684b7ab7be88727969fdad": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e875e3a90f7c4130a4c347cd5db39bcb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9861695659994af89cfe86736629285a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}